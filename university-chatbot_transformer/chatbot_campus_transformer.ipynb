{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd8ddC7NQ8uZ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## -- Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb_5bl7G_n30",
        "outputId": "f5efb05f-1816-4b06-fc2b-e0f1b49c9d0d",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\Programs\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "Tensorflow version 2.15.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorflow==2.9.1 tensorflow_datasets==4.6.0\n",
        "\n",
        "# Python built-in imports\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "from time import time\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import spacy\n",
        "\n",
        "# TensorFlow specific setup\n",
        "tf.keras.utils.set_random_seed(1234)\n",
        "\n",
        "# Version check\n",
        "print(f\"Tensorflow version {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YXFrSnNE_2j",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## -- GPU / TPU initialization (Colab Only)\n",
        "\n",
        "On Google Colab, select `TPU` or `GPU` hardware accelerator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz2ZbpAREmRA",
        "outputId": "976f68bd-ffa1-492b-d1e8-410b29858ef6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "not run in colab\n"
          ]
        }
      ],
      "source": [
        "def is_colab():\n",
        "    return 'google.colab' in sys.modules\n",
        "\n",
        "if is_colab():\n",
        "    print('run in colab')\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print(\"Running on TPU {}\".format(tpu.cluster_spec().as_dict()[\"worker\"]))\n",
        "    except ValueError:\n",
        "        tpu = None\n",
        "\n",
        "    if tpu:\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    else:\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "\n",
        "    print(f\"REPLICAS: {strategy.num_replicas_in_sync}\")\n",
        "else:\n",
        "    print('not run in colab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skMfzRV5cEnb",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## -- Variable Initialization\n",
        "\n",
        "To keep this example small and relatively fast, the values for *num_layers, d_model, and units* have been reduced. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "alnYEatYcHp7",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Maximum sentence length\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "# Maximum number of samples to preprocess\n",
        "MAX_SAMPLES = 50000\n",
        "\n",
        "# For tf.data.Dataset\n",
        "if not is_colab():\n",
        "    BATCH_SIZE = 512\n",
        "else:\n",
        "    BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# For Transformer\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "UNITS = 512 #nodes or neuron\n",
        "DROPOUT = 0.1 #// 0.2 // 0.5\n",
        "\n",
        "EPOCHS = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0AqALdZCbCW",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample:\n",
            "Q: Hi\n",
            "A: Hello!\n",
            "\n",
            "Q: Hi\n",
            "A: Hello, $_user!\n",
            "\n",
            "Q: Hi\n",
            "A: Good to see you again!\n",
            "\n",
            "Q: Hi\n",
            "A: Hi there, how can I help?\n",
            "\n",
            "Q: Hi\n",
            "A: Hello!\n",
            "\n",
            "Q: Hi\n",
            "A: Hello, $_user!\n",
            "\n",
            "Total questions: 542\n",
            "Total answers: 542\n",
            "Unique questions: 397\n",
            "Unique answers: 50\n"
          ]
        }
      ],
      "source": [
        "# Load the intent JSON\n",
        "with open(\"../intents.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "# For each pattern, pair it with every response (1:many)\n",
        "for intent in data['intents']:\n",
        "    patterns = intent.get('patterns', [])\n",
        "    responses = intent.get('responses', [])\n",
        "\n",
        "    for pattern in patterns:\n",
        "        for response in responses:\n",
        "            questions.append(pattern)\n",
        "            answers.append(response)\n",
        "\n",
        "# Sample output\n",
        "print(\"Sample:\")\n",
        "for q, a in list(zip(questions, answers))[:6]:\n",
        "    print(f\"Q: {q}\\nA: {a}\\n\")\n",
        "\n",
        "# Summary of questions and answers\n",
        "n_question = len(questions)\n",
        "n_answer = len(answers)\n",
        "unique_questions = len(set(questions))\n",
        "unique_answers = len(set(answers))\n",
        "\n",
        "print(f\"Total questions: {n_question}\")\n",
        "print(f\"Total answers: {n_answer}\")\n",
        "print(f\"Unique questions: {unique_questions}\")\n",
        "print(f\"Unique answers: {unique_answers}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## -- Common Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    # removing contractions\n",
        "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
        "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
        "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
        "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
        "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
        "    sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
        "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
        "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
        "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
        "    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
        "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
        "    sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
        "    sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== question summary =====\n",
            "Original number of texts: 542\n",
            "Number of texts after preprocessing: 538\n",
            "Number of skipped texts: 4\n",
            "Sample before preprocessing: ['Hi', 'Hi', 'Hi', 'Hi', 'Hi']\n",
            "Sample after preprocessing: ['hi', 'hi', 'hi', 'hi', 'hi']\n",
            "\n",
            "===== answer summary =====\n",
            "Original number of labels: 542\n",
            "Number of labels after preprocessing: 538\n",
            "Sample labels before preprocessing: ['hello!', 'hello, $_user!', 'good to see you again!', 'hi there, how can i help?', 'hello!']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text_per_word(text):\n",
        "    words = text.split()\n",
        "    processed_words = [word.lower() for word in words]\n",
        "    processed_words = [word.strip() for word in processed_words]\n",
        "    processed_words = [re.sub('\\s+',' ', word) for word in processed_words]\n",
        "    processed_words = [word for word in processed_words if not all(char in string.punctuation for char in word.replace(' ',''))]\n",
        "    \n",
        "    processed_words = ' '.join(processed_words)\n",
        "    return processed_words\n",
        "\n",
        "def filter_and_skip(preprocessed_texts):\n",
        "    filtered_texts = []\n",
        "    skipped_indices = []\n",
        "    for i, word in enumerate(preprocessed_texts):\n",
        "        if word.strip() != '':\n",
        "            filtered_texts.append(word)\n",
        "        else:\n",
        "            skipped_indices.append(i)\n",
        "    return filtered_texts, skipped_indices\n",
        "\n",
        "preprocessed_questions = [preprocess_text_per_word(text) for text in questions]\n",
        "preprocessed_answers = [preprocess_text_per_word(text) for text in answers]\n",
        "\n",
        "preprocessed_questions, skipped_indices = filter_and_skip(preprocessed_questions)\n",
        "preprocessed_answers = [answer for i, answer in enumerate(preprocessed_answers) if i not in skipped_indices]\n",
        "\n",
        "\n",
        "# summary\n",
        "print(\"===== question summary =====\")\n",
        "print(f\"Original number of texts: {len(questions)}\")\n",
        "print(f\"Number of texts after preprocessing: {len(preprocessed_questions)}\")\n",
        "print(f\"Number of skipped texts: {len(skipped_indices)}\")\n",
        "print(\"Sample before preprocessing:\", questions[:5])\n",
        "print(\"Sample after preprocessing:\", preprocessed_questions[:5])\n",
        "print()\n",
        "print(\"===== answer summary =====\")\n",
        "print(f\"Original number of labels: {len(preprocessed_answers) + len(skipped_indices)}\")\n",
        "print(f\"Number of labels after preprocessing: {len(preprocessed_answers)}\")\n",
        "print(\"Sample labels before preprocessing:\", preprocessed_answers[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## -- Semantic Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "NER Tagging Summary:\n",
            "----------------------------------------\n",
            "Total questions processed: 538\n",
            "before\n",
            "['hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'how are you?', 'how are you?']\n",
            "\n",
            "after\n",
            "['hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'how are you ?', 'how are you ?']\n"
          ]
        }
      ],
      "source": [
        "def apply_ner_tags(text):\n",
        "    doc = nlp(text)\n",
        "    \n",
        "    tagged_tokens = []\n",
        "    for token in doc:\n",
        "        replaced = False\n",
        "        for ent in doc.ents:\n",
        "            if token.text == ent.text:\n",
        "                tagged_tokens.append(f\"<{ent.label_}>\")\n",
        "                replaced = True\n",
        "                break\n",
        "        if not replaced:\n",
        "            tagged_tokens.append(token.text)\n",
        "    return \" \".join(tagged_tokens)\n",
        "\n",
        "# Keep original preprocessed questions before NER tagging\n",
        "original_questions = preprocessed_questions.copy()\n",
        "\n",
        "# Apply NER tags to questions\n",
        "preprocessed_questions = [apply_ner_tags(text) for text in preprocessed_questions]\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nNER Tagging Summary:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Total questions processed: {len(preprocessed_questions)}\")\n",
        "\n",
        "print('before')\n",
        "print(original_questions[:10])\n",
        "print()\n",
        "print('after')\n",
        "print(preprocessed_questions[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprataion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## -- Build Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "s6XX2udMTCQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized sample question: [617, 657, 657, 646, 578, 236]\n"
          ]
        }
      ],
      "source": [
        "# Build tokenizer using tfds for both questions and answers\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    preprocessed_questions + preprocessed_answers, target_vocab_size=2**13\n",
        ")\n",
        "\n",
        "# Define start and end token to indicate the start and end of a sentence\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "# Vocabulary size plus start and end token\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "\n",
        "print(f\"Tokenized sample question: {tokenizer.encode(questions[20])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## -- Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YESTPgeg_XgT",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Tokenize, filter and pad sentences\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "        # tokenize sentence\n",
        "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "        # check tokenized sentence max length\n",
        "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "            tokenized_inputs.append(sentence1)\n",
        "            tokenized_outputs.append(sentence2)\n",
        "\n",
        "    # pad tokenized sentences\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_inputs, maxlen=MAX_LENGTH, padding=\"post\"\n",
        "    )\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_outputs, maxlen=MAX_LENGTH, padding=\"post\"\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "\n",
        "preprocessed_questions, preprocessed_answers = tokenize_and_filter(preprocessed_questions, preprocessed_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pohHm8IRWlIH",
        "outputId": "fea76d83-a7d3-4a9e-8aef-a5fd07fdc6dd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 804\n",
            "Number of samples: 542\n"
          ]
        }
      ],
      "source": [
        "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
        "print(f\"Number of samples: {len(questions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## -- Seq2Seq Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pttC3XxgAXWQ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_PrefetchDataset element_spec=({'inputs': TensorSpec(shape=(None, 40), dtype=tf.int32, name=None), 'dec_inputs': TensorSpec(shape=(None, 39), dtype=tf.int32, name=None)}, {'outputs': TensorSpec(shape=(None, 39), dtype=tf.int32, name=None)})>\n"
          ]
        }
      ],
      "source": [
        "# decoder inputs use the previous target as input\n",
        "# remove START_TOKEN from targets\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "        {\"inputs\": preprocessed_questions, \"dec_inputs\": preprocessed_answers[:, :-1]},\n",
        "        {\"outputs\": preprocessed_answers[:, 1:]},\n",
        "    )\n",
        ")\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "538"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(preprocessed_answers[:, :-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9eeMPjGXmI1",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## -- Attention\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uctkwvPZVSzu",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Scaled dot product Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "ENfqAFna_50H",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    \"\"\"Calculate the attention weights.\"\"\"\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    # add the mask to zero out padding tokens\n",
        "    if mask is not None:\n",
        "        logits += mask * -1e9\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k)\n",
        "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwmOB9HvVbyh",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "L9eYssGIAG4h",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, **kwargs):\n",
        "        assert d_model % num_heads == 0\n",
        "        super(MultiHeadAttentionLayer, self).__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(MultiHeadAttentionLayer, self).get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"d_model\": self.d_model,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        inputs = tf.keras.layers.Lambda(\n",
        "            lambda inputs: tf.reshape(\n",
        "                inputs, shape=(batch_size, -1, self.num_heads, self.depth)\n",
        "            )\n",
        "        )(inputs)\n",
        "        return tf.keras.layers.Lambda(\n",
        "            lambda inputs: tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "        )(inputs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = (\n",
        "            inputs[\"query\"],\n",
        "            inputs[\"key\"],\n",
        "            inputs[\"value\"],\n",
        "            inputs[\"mask\"],\n",
        "        )\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        # linear layers\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # split heads\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # scaled dot-product attention\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "        scaled_attention = tf.keras.layers.Lambda(\n",
        "            lambda scaled_attention: tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        )(scaled_attention)\n",
        "\n",
        "        # concatenation of heads\n",
        "        concat_attention = tf.keras.layers.Lambda(\n",
        "            lambda scaled_attention: tf.reshape(\n",
        "                scaled_attention, (batch_size, -1, self.d_model)\n",
        "            )\n",
        "        )(scaled_attention)\n",
        "\n",
        "        # final linear layer\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDUX7Oa8Xudj",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## -- Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5QlgXsxYirg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Masking\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "imCQ0jrvWhC7",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(x):\n",
        "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "    # (batch_size, 1, 1, sequence length)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrwtsqrfWd-3",
        "outputId": "b9f68588-73e0-48c0-fb14-cf027bbb7317",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[[0. 0. 1. 0. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 0. 0.]]]], shape=(2, 1, 1, 5), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "HSVdD2zKWaXx",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(x):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(x)\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhwz9xzxWcod",
        "outputId": "9ffdb312-65c6-4335-a77a-898485cf96af",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[[0. 1. 1. 1. 1.]\n",
            "   [0. 0. 1. 1. 1.]\n",
            "   [0. 0. 1. 1. 1.]\n",
            "   [0. 0. 1. 0. 1.]\n",
            "   [0. 0. 1. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpR7kz4jFkPJ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "-9Oibz2es-qW",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.position = position\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"position\": self.position,\n",
        "                \"d_model\": self.d_model,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model,\n",
        "        )\n",
        "        # apply sin to even index in the array\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        # apply cos to odd index in the array\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, : tf.shape(inputs)[1], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "UC_fQehi3_Yh",
        "outputId": "75a3d09a-3aad-4376-a634-4f9d2cc8c4d2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sample_pos_encoding = PositionalEncoding(50, 512, name=\"sample_pos_encoding\")\n",
        "\n",
        "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap=\"RdBu\")\n",
        "plt.xlabel(\"Depth\")\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel(\"Position\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVazCemoW2Ye",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "5guJOLJmfcuX",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    attention = MultiHeadAttentionLayer(d_model, num_heads, name=\"attention\")(\n",
        "        {\"query\": inputs, \"key\": inputs, \"value\": inputs, \"mask\": padding_mask}\n",
        "    )\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    add_attention = tf.keras.layers.add([inputs, attention])\n",
        "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation=\"relu\")(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    add_attention = tf.keras.layers.add([attention, outputs])\n",
        "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K16BIGSKfkve",
        "outputId": "bb33d93c-f54a-4c08-814d-60749980a4dc",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "sample_encoder_layer = encoder_layer(\n",
        "    units=512, d_model=128, num_heads=4, dropout=0.3, name=\"sample_encoder_layer\"\n",
        ")\n",
        "\n",
        "tf.keras.utils.plot_model(\n",
        "    sample_encoder_layer, to_file=\"encoder_layer.png\", show_shapes=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r8lWGClfi_1",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "LRfugon5Wy-Y",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def encoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"encoder\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.keras.layers.Lambda(\n",
        "        lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    )(d_model)\n",
        "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name=\"encoder_layer_{}\".format(i),\n",
        "        )([outputs, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "bNxCnjrvglnx",
        "outputId": "59311348-2c31-42d3-acef-da08e723c7ff",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "sample_encoder = encoder(\n",
        "    vocab_size=8192,\n",
        "    num_layers=2,\n",
        "    units=512,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    dropout=0.3,\n",
        "    name=\"sample_encoder\",\n",
        ")\n",
        "\n",
        "tf.keras.utils.plot_model(sample_encoder, to_file=\"encoder.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af66azvgW9P-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "6mLvvNMWgDnf",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    attention1 = MultiHeadAttentionLayer(d_model, num_heads, name=\"attention_1\")(\n",
        "        inputs={\n",
        "            \"query\": inputs,\n",
        "            \"key\": inputs,\n",
        "            \"value\": inputs,\n",
        "            \"mask\": look_ahead_mask,\n",
        "        }\n",
        "    )\n",
        "    add_attention = tf.keras.layers.add([attention1, inputs])\n",
        "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
        "\n",
        "    attention2 = MultiHeadAttentionLayer(d_model, num_heads, name=\"attention_2\")(\n",
        "        inputs={\n",
        "            \"query\": attention1,\n",
        "            \"key\": enc_outputs,\n",
        "            \"value\": enc_outputs,\n",
        "            \"mask\": padding_mask,\n",
        "        }\n",
        "    )\n",
        "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    add_attention = tf.keras.layers.add([attention2, attention1])\n",
        "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation=\"relu\")(attention2)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    add_attention = tf.keras.layers.add([outputs, attention2])\n",
        "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "        outputs=outputs,\n",
        "        name=name,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8M1NrQ_NgEaM",
        "outputId": "2d0284ec-9276-433e-a2b6-38358448bbfe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "sample_decoder_layer = decoder_layer(\n",
        "    units=512, d_model=128, num_heads=4, dropout=0.3, name=\"sample_decoder_layer\"\n",
        ")\n",
        "\n",
        "tf.keras.utils.plot_model(\n",
        "    sample_decoder_layer, to_file=\"decoder_layer.png\", show_shapes=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPSKnjS-gE_q",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "dYRx7YzCW4bu",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"decoder\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.keras.layers.Lambda(\n",
        "        lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    )(d_model)\n",
        "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name=\"decoder_layer_{}\".format(i),\n",
        "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "        outputs=outputs,\n",
        "        name=name,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "tUdK8jb9hlTZ",
        "outputId": "f9e5b1e3-e692-4cb6-b650-20f0a1e2d6c4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "sample_decoder = decoder(\n",
        "    vocab_size=8192,\n",
        "    num_layers=2,\n",
        "    units=512,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    dropout=0.3,\n",
        "    name=\"sample_decoder\",\n",
        ")\n",
        "\n",
        "tf.keras.utils.plot_model(sample_decoder, to_file=\"decoder.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl0o97RJXAqw",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "TW-v7Fz6XAfC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def transformer(\n",
        "    vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"transformer\"\n",
        "):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "        create_padding_mask, output_shape=(1, 1, None), name=\"enc_padding_mask\"\n",
        "    )(inputs)\n",
        "    # mask the future tokens for decoder inputs at the 1st attention block\n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "        create_look_ahead_mask, output_shape=(1, None, None), name=\"look_ahead_mask\"\n",
        "    )(dec_inputs)\n",
        "    # mask the encoder outputs for the 2nd attention block\n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "        create_padding_mask, output_shape=(1, 1, None), name=\"dec_padding_mask\"\n",
        "    )(inputs)\n",
        "\n",
        "    enc_outputs = encoder(\n",
        "        vocab_size=vocab_size,\n",
        "        num_layers=num_layers,\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "    )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    dec_outputs = decoder(\n",
        "        vocab_size=vocab_size,\n",
        "        num_layers=num_layers,\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "aihJLVq_iJ_T",
        "outputId": "68ace56a-5e77-4dcd-e680-0ea46056bd4a",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "sample_transformer = transformer(\n",
        "    vocab_size=8192,\n",
        "    num_layers=4,\n",
        "    units=512,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    dropout=0.3,\n",
        "    name=\"sample_transformer\",\n",
        ")\n",
        "\n",
        "tf.keras.utils.plot_model(\n",
        "    sample_transformer, to_file=\"transformer.png\", show_shapes=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HD7GK-nh_KT",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## -- Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_GCb0LaV1tI",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "UInVM9iGAMv1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction=\"none\"\n",
        "    )(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvFM9ajSVybP",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Custom learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "WW3SeLDhAMJd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = float(d_model)  # use float to avoid save issues\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"d_model\": self.d_model,\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "        }\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "67BoG_UeaHHw",
        "outputId": "a47cc17d-37c7-4d09-a44f-43577606072e",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sample_learning_rate = CustomSchedule(d_model=128)\n",
        "\n",
        "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCqve3kwWCxd",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Initialize and compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QqojIa5WEQq",
        "outputId": "6969c2ea-a418-4ca4-84ab-04ef4502df0e",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "creating model architecture ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compiling model ...\n",
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " inputs (InputLayer)         [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " dec_inputs (InputLayer)     [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " enc_padding_mask (Lambda)   (None, 1, 1, None)           0         ['inputs[0][0]']              \n",
            "                                                                                                  \n",
            " encoder (Functional)        (None, None, 256)            1260032   ['inputs[0][0]',              \n",
            "                                                                     'enc_padding_mask[0][0]']    \n",
            "                                                                                                  \n",
            " look_ahead_mask (Lambda)    (None, 1, None, None)        0         ['dec_inputs[0][0]']          \n",
            "                                                                                                  \n",
            " dec_padding_mask (Lambda)   (None, 1, 1, None)           0         ['inputs[0][0]']              \n",
            "                                                                                                  \n",
            " decoder (Functional)        (None, None, 256)            1787392   ['dec_inputs[0][0]',          \n",
            "                                                                     'encoder[0][0]',             \n",
            "                                                                     'look_ahead_mask[0][0]',     \n",
            "                                                                     'dec_padding_mask[0][0]']    \n",
            "                                                                                                  \n",
            " outputs (Dense)             (None, None, 804)            206628    ['decoder[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3254052 (12.41 MB)\n",
            "Trainable params: 3254052 (12.41 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# clear backend\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
        ")\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
        "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "\n",
        "# initialize and compile model within strategy scope\n",
        "with strategy.scope():\n",
        "    print('creating model architecture ...')\n",
        "    model = transformer(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        units=UNITS,\n",
        "        d_model=D_MODEL,\n",
        "        num_heads=NUM_HEADS,\n",
        "        dropout=DROPOUT,\n",
        "    )\n",
        "\n",
        "    print('compiling model ...')\n",
        "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDMd69urLNuc",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Fit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7iahRzlLNG2",
        "outputId": "dee60749-0cd7-46ec-83dd-08b4d49f1dc1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "9/9 [==============================] - 54s 1s/step - loss: 2.5180 - accuracy: 4.7660e-04\n",
            "Epoch 2/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 2.5051 - accuracy: 9.0554e-04\n",
            "Epoch 3/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 2.4776 - accuracy: 0.0017\n",
            "Epoch 4/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 2.4416 - accuracy: 0.0094\n",
            "Epoch 5/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 2.3961 - accuracy: 0.0213\n",
            "Epoch 6/1000\n",
            "9/9 [==============================] - 9s 959ms/step - loss: 2.3481 - accuracy: 0.0253\n",
            "Epoch 7/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 2.3041 - accuracy: 0.0255\n",
            "Epoch 8/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 2.2631 - accuracy: 0.0254\n",
            "Epoch 9/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 2.2218 - accuracy: 0.0257\n",
            "Epoch 10/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 2.1847 - accuracy: 0.0256\n",
            "Epoch 11/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 2.1467 - accuracy: 0.0259\n",
            "Epoch 12/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 2.1080 - accuracy: 0.0259\n",
            "Epoch 13/1000\n",
            "9/9 [==============================] - 13s 1s/step - loss: 2.0686 - accuracy: 0.0269\n",
            "Epoch 14/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 2.0267 - accuracy: 0.0282\n",
            "Epoch 15/1000\n",
            "9/9 [==============================] - 9s 965ms/step - loss: 1.9786 - accuracy: 0.0356\n",
            "Epoch 16/1000\n",
            "9/9 [==============================] - 9s 952ms/step - loss: 1.9191 - accuracy: 0.0456\n",
            "Epoch 17/1000\n",
            "9/9 [==============================] - 9s 976ms/step - loss: 1.8555 - accuracy: 0.0660\n",
            "Epoch 18/1000\n",
            "9/9 [==============================] - 9s 973ms/step - loss: 1.7885 - accuracy: 0.0865\n",
            "Epoch 19/1000\n",
            "9/9 [==============================] - 9s 963ms/step - loss: 1.7143 - accuracy: 0.1059\n",
            "Epoch 20/1000\n",
            "9/9 [==============================] - 9s 965ms/step - loss: 1.6354 - accuracy: 0.1232\n",
            "Epoch 21/1000\n",
            "9/9 [==============================] - 9s 977ms/step - loss: 1.5542 - accuracy: 0.1359\n",
            "Epoch 22/1000\n",
            "9/9 [==============================] - 9s 979ms/step - loss: 1.4754 - accuracy: 0.1444\n",
            "Epoch 23/1000\n",
            "9/9 [==============================] - 9s 968ms/step - loss: 1.3940 - accuracy: 0.1559\n",
            "Epoch 24/1000\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 1.3165 - accuracy: 0.1664\n",
            "Epoch 25/1000\n",
            "9/9 [==============================] - 9s 955ms/step - loss: 1.2460 - accuracy: 0.1774\n",
            "Epoch 26/1000\n",
            "9/9 [==============================] - 9s 942ms/step - loss: 1.1727 - accuracy: 0.1854\n",
            "Epoch 27/1000\n",
            "9/9 [==============================] - 9s 943ms/step - loss: 1.1042 - accuracy: 0.1984\n",
            "Epoch 28/1000\n",
            "9/9 [==============================] - 8s 925ms/step - loss: 1.0427 - accuracy: 0.2077\n",
            "Epoch 29/1000\n",
            "9/9 [==============================] - 9s 969ms/step - loss: 0.9816 - accuracy: 0.2169\n",
            "Epoch 30/1000\n",
            "9/9 [==============================] - 9s 962ms/step - loss: 0.9247 - accuracy: 0.2263\n",
            "Epoch 31/1000\n",
            "9/9 [==============================] - 9s 941ms/step - loss: 0.8668 - accuracy: 0.2392\n",
            "Epoch 32/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.8106 - accuracy: 0.2510\n",
            "Epoch 33/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.7585 - accuracy: 0.2568\n",
            "Epoch 34/1000\n",
            "9/9 [==============================] - 8s 925ms/step - loss: 0.7089 - accuracy: 0.2683\n",
            "Epoch 35/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.6655 - accuracy: 0.2744\n",
            "Epoch 36/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.6196 - accuracy: 0.2839\n",
            "Epoch 37/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.5785 - accuracy: 0.2918\n",
            "Epoch 38/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.5389 - accuracy: 0.2955\n",
            "Epoch 39/1000\n",
            "9/9 [==============================] - 8s 916ms/step - loss: 0.4991 - accuracy: 0.3034\n",
            "Epoch 40/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.4661 - accuracy: 0.3066\n",
            "Epoch 41/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.4309 - accuracy: 0.3110\n",
            "Epoch 42/1000\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 0.4027 - accuracy: 0.3153\n",
            "Epoch 43/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.3746 - accuracy: 0.3174\n",
            "Epoch 44/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.3467 - accuracy: 0.3236\n",
            "Epoch 45/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.3233 - accuracy: 0.3258\n",
            "Epoch 46/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.2995 - accuracy: 0.3270\n",
            "Epoch 47/1000\n",
            "9/9 [==============================] - 9s 964ms/step - loss: 0.2782 - accuracy: 0.3318\n",
            "Epoch 48/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.2589 - accuracy: 0.3344\n",
            "Epoch 49/1000\n",
            "9/9 [==============================] - 8s 922ms/step - loss: 0.2401 - accuracy: 0.3369\n",
            "Epoch 50/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.2249 - accuracy: 0.3392\n",
            "Epoch 51/1000\n",
            "9/9 [==============================] - 8s 922ms/step - loss: 0.2071 - accuracy: 0.3415\n",
            "Epoch 52/1000\n",
            "9/9 [==============================] - 9s 940ms/step - loss: 0.1926 - accuracy: 0.3433\n",
            "Epoch 53/1000\n",
            "9/9 [==============================] - 9s 935ms/step - loss: 0.1790 - accuracy: 0.3445\n",
            "Epoch 54/1000\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.1660 - accuracy: 0.3462\n",
            "Epoch 55/1000\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.1532 - accuracy: 0.3480\n",
            "Epoch 56/1000\n",
            "9/9 [==============================] - 9s 950ms/step - loss: 0.1406 - accuracy: 0.3496\n",
            "Epoch 57/1000\n",
            "9/9 [==============================] - 9s 954ms/step - loss: 0.1313 - accuracy: 0.3507\n",
            "Epoch 58/1000\n",
            "9/9 [==============================] - 8s 930ms/step - loss: 0.1203 - accuracy: 0.3521\n",
            "Epoch 59/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.1103 - accuracy: 0.3536\n",
            "Epoch 60/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0999 - accuracy: 0.3554\n",
            "Epoch 61/1000\n",
            "9/9 [==============================] - 9s 950ms/step - loss: 0.0917 - accuracy: 0.3563\n",
            "Epoch 62/1000\n",
            "9/9 [==============================] - 9s 980ms/step - loss: 0.0831 - accuracy: 0.3579\n",
            "Epoch 63/1000\n",
            "9/9 [==============================] - 9s 960ms/step - loss: 0.0771 - accuracy: 0.3580\n",
            "Epoch 64/1000\n",
            "9/9 [==============================] - 9s 946ms/step - loss: 0.0701 - accuracy: 0.3597\n",
            "Epoch 65/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0647 - accuracy: 0.3607\n",
            "Epoch 66/1000\n",
            "9/9 [==============================] - 9s 980ms/step - loss: 0.0596 - accuracy: 0.3605\n",
            "Epoch 67/1000\n",
            "9/9 [==============================] - 9s 980ms/step - loss: 0.0535 - accuracy: 0.3614\n",
            "Epoch 68/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0487 - accuracy: 0.3621\n",
            "Epoch 69/1000\n",
            "9/9 [==============================] - 9s 992ms/step - loss: 0.0458 - accuracy: 0.3619\n",
            "Epoch 70/1000\n",
            "9/9 [==============================] - 9s 984ms/step - loss: 0.0412 - accuracy: 0.3627\n",
            "Epoch 71/1000\n",
            "9/9 [==============================] - 9s 994ms/step - loss: 0.0383 - accuracy: 0.3631\n",
            "Epoch 72/1000\n",
            "9/9 [==============================] - 9s 984ms/step - loss: 0.0354 - accuracy: 0.3628\n",
            "Epoch 73/1000\n",
            "9/9 [==============================] - 9s 941ms/step - loss: 0.0341 - accuracy: 0.3631\n",
            "Epoch 74/1000\n",
            "9/9 [==============================] - 8s 922ms/step - loss: 0.0322 - accuracy: 0.3636\n",
            "Epoch 75/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0302 - accuracy: 0.3630\n",
            "Epoch 76/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0285 - accuracy: 0.3630\n",
            "Epoch 77/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0269 - accuracy: 0.3635\n",
            "Epoch 78/1000\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0250 - accuracy: 0.3637\n",
            "Epoch 79/1000\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0241 - accuracy: 0.3633\n",
            "Epoch 80/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0235 - accuracy: 0.3633\n",
            "Epoch 81/1000\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0228 - accuracy: 0.3631\n",
            "Epoch 82/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0216 - accuracy: 0.3638\n",
            "Epoch 83/1000\n",
            "9/9 [==============================] - 8s 921ms/step - loss: 0.0204 - accuracy: 0.3637\n",
            "Epoch 84/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0197 - accuracy: 0.3641\n",
            "Epoch 85/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0194 - accuracy: 0.3633\n",
            "Epoch 86/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0189 - accuracy: 0.3638\n",
            "Epoch 87/1000\n",
            "9/9 [==============================] - 8s 916ms/step - loss: 0.0174 - accuracy: 0.3641\n",
            "Epoch 88/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0171 - accuracy: 0.3635\n",
            "Epoch 89/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0181 - accuracy: 0.3637\n",
            "Epoch 90/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0178 - accuracy: 0.3636\n",
            "Epoch 91/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0172 - accuracy: 0.3637\n",
            "Epoch 92/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0164 - accuracy: 0.3639\n",
            "Epoch 93/1000\n",
            "9/9 [==============================] - 8s 929ms/step - loss: 0.0155 - accuracy: 0.3638\n",
            "Epoch 94/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0159 - accuracy: 0.3634\n",
            "Epoch 95/1000\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0159 - accuracy: 0.3637\n",
            "Epoch 96/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0149 - accuracy: 0.3642\n",
            "Epoch 97/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0147 - accuracy: 0.3645\n",
            "Epoch 98/1000\n",
            "9/9 [==============================] - 8s 925ms/step - loss: 0.0146 - accuracy: 0.3633\n",
            "Epoch 99/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0148 - accuracy: 0.3638\n",
            "Epoch 100/1000\n",
            "9/9 [==============================] - 8s 924ms/step - loss: 0.0144 - accuracy: 0.3641\n",
            "Epoch 101/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0152 - accuracy: 0.3637\n",
            "Epoch 102/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0142 - accuracy: 0.3636\n",
            "Epoch 103/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0137 - accuracy: 0.3637\n",
            "Epoch 104/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0145 - accuracy: 0.3634\n",
            "Epoch 105/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0138 - accuracy: 0.3639\n",
            "Epoch 106/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0134 - accuracy: 0.3642\n",
            "Epoch 107/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0139 - accuracy: 0.3635\n",
            "Epoch 108/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0136 - accuracy: 0.3642\n",
            "Epoch 109/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0137 - accuracy: 0.3643\n",
            "Epoch 110/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0145 - accuracy: 0.3634\n",
            "Epoch 111/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0136 - accuracy: 0.3630\n",
            "Epoch 112/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0138 - accuracy: 0.3639\n",
            "Epoch 113/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0137 - accuracy: 0.3634\n",
            "Epoch 114/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0137 - accuracy: 0.3634\n",
            "Epoch 115/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0130 - accuracy: 0.3641\n",
            "Epoch 116/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0126 - accuracy: 0.3640\n",
            "Epoch 117/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0132 - accuracy: 0.3637\n",
            "Epoch 118/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0133 - accuracy: 0.3638\n",
            "Epoch 119/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0139 - accuracy: 0.3635\n",
            "Epoch 120/1000\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0128 - accuracy: 0.3642\n",
            "Epoch 121/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0127 - accuracy: 0.3640\n",
            "Epoch 122/1000\n",
            "9/9 [==============================] - 8s 923ms/step - loss: 0.0126 - accuracy: 0.3645\n",
            "Epoch 123/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0126 - accuracy: 0.3637\n",
            "Epoch 124/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0128 - accuracy: 0.3634\n",
            "Epoch 125/1000\n",
            "9/9 [==============================] - 8s 916ms/step - loss: 0.0129 - accuracy: 0.3637\n",
            "Epoch 126/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0126 - accuracy: 0.3635\n",
            "Epoch 127/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0127 - accuracy: 0.3641\n",
            "Epoch 128/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0126 - accuracy: 0.3639\n",
            "Epoch 129/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0125 - accuracy: 0.3642\n",
            "Epoch 130/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0129 - accuracy: 0.3641\n",
            "Epoch 131/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0124 - accuracy: 0.3640\n",
            "Epoch 132/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0126 - accuracy: 0.3642\n",
            "Epoch 133/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0125 - accuracy: 0.3636\n",
            "Epoch 134/1000\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0130 - accuracy: 0.3640\n",
            "Epoch 135/1000\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0128 - accuracy: 0.3639\n",
            "Epoch 136/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0130 - accuracy: 0.3639\n",
            "Epoch 137/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0128 - accuracy: 0.3635\n",
            "Epoch 138/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0128 - accuracy: 0.3638\n",
            "Epoch 139/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0132 - accuracy: 0.3634\n",
            "Epoch 140/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0126 - accuracy: 0.3642\n",
            "Epoch 141/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0128 - accuracy: 0.3637\n",
            "Epoch 142/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0127 - accuracy: 0.3636\n",
            "Epoch 143/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0121 - accuracy: 0.3645\n",
            "Epoch 144/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0122 - accuracy: 0.3637\n",
            "Epoch 145/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0122 - accuracy: 0.3640\n",
            "Epoch 146/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0124 - accuracy: 0.3630\n",
            "Epoch 147/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 148/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 149/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0126 - accuracy: 0.3638\n",
            "Epoch 150/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0122 - accuracy: 0.3640\n",
            "Epoch 151/1000\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 0.0126 - accuracy: 0.3634\n",
            "Epoch 152/1000\n",
            "9/9 [==============================] - 8s 930ms/step - loss: 0.0120 - accuracy: 0.3641\n",
            "Epoch 153/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0120 - accuracy: 0.3636\n",
            "Epoch 154/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0120 - accuracy: 0.3643\n",
            "Epoch 155/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0123 - accuracy: 0.3632\n",
            "Epoch 156/1000\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0122 - accuracy: 0.3640\n",
            "Epoch 157/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0122 - accuracy: 0.3638\n",
            "Epoch 158/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0120 - accuracy: 0.3641\n",
            "Epoch 159/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0123 - accuracy: 0.3640\n",
            "Epoch 160/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0124 - accuracy: 0.3639\n",
            "Epoch 161/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0125 - accuracy: 0.3639\n",
            "Epoch 162/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0122 - accuracy: 0.3639\n",
            "Epoch 163/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0118 - accuracy: 0.3642\n",
            "Epoch 164/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0121 - accuracy: 0.3637\n",
            "Epoch 165/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0121 - accuracy: 0.3642\n",
            "Epoch 166/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0120 - accuracy: 0.3641\n",
            "Epoch 167/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0121 - accuracy: 0.3638\n",
            "Epoch 168/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0128 - accuracy: 0.3632\n",
            "Epoch 169/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0124 - accuracy: 0.3636\n",
            "Epoch 170/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0126 - accuracy: 0.3640\n",
            "Epoch 171/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0123 - accuracy: 0.3640\n",
            "Epoch 172/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0132 - accuracy: 0.3636\n",
            "Epoch 173/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0127 - accuracy: 0.3635\n",
            "Epoch 174/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0122 - accuracy: 0.3642\n",
            "Epoch 175/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0119 - accuracy: 0.3641\n",
            "Epoch 176/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0119 - accuracy: 0.3642\n",
            "Epoch 177/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0120 - accuracy: 0.3643\n",
            "Epoch 178/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 179/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 180/1000\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0119 - accuracy: 0.3642\n",
            "Epoch 181/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 182/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0120 - accuracy: 0.3644\n",
            "Epoch 183/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 184/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0119 - accuracy: 0.3635\n",
            "Epoch 185/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0121 - accuracy: 0.3639\n",
            "Epoch 186/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 187/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0120 - accuracy: 0.3644\n",
            "Epoch 188/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0118 - accuracy: 0.3643\n",
            "Epoch 189/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0119 - accuracy: 0.3636\n",
            "Epoch 190/1000\n",
            "9/9 [==============================] - 8s 916ms/step - loss: 0.0124 - accuracy: 0.3640\n",
            "Epoch 191/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0121 - accuracy: 0.3640\n",
            "Epoch 192/1000\n",
            "9/9 [==============================] - 8s 921ms/step - loss: 0.0118 - accuracy: 0.3644\n",
            "Epoch 193/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0120 - accuracy: 0.3640\n",
            "Epoch 194/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0120 - accuracy: 0.3637\n",
            "Epoch 195/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0118 - accuracy: 0.3636\n",
            "Epoch 196/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0122 - accuracy: 0.3637\n",
            "Epoch 197/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0126 - accuracy: 0.3640\n",
            "Epoch 198/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0125 - accuracy: 0.3638\n",
            "Epoch 199/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0129 - accuracy: 0.3639\n",
            "Epoch 200/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0125 - accuracy: 0.3640\n",
            "Epoch 201/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0124 - accuracy: 0.3638\n",
            "Epoch 202/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0138 - accuracy: 0.3633\n",
            "Epoch 203/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0126 - accuracy: 0.3635\n",
            "Epoch 204/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0120 - accuracy: 0.3642\n",
            "Epoch 205/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0120 - accuracy: 0.3641\n",
            "Epoch 206/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0120 - accuracy: 0.3634\n",
            "Epoch 207/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0119 - accuracy: 0.3646\n",
            "Epoch 208/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 209/1000\n",
            "9/9 [==============================] - 8s 931ms/step - loss: 0.0122 - accuracy: 0.3640\n",
            "Epoch 210/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0120 - accuracy: 0.3642\n",
            "Epoch 211/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0117 - accuracy: 0.3645\n",
            "Epoch 212/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0120 - accuracy: 0.3640\n",
            "Epoch 213/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0116 - accuracy: 0.3642\n",
            "Epoch 214/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0120 - accuracy: 0.3640\n",
            "Epoch 215/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0119 - accuracy: 0.3641\n",
            "Epoch 216/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0120 - accuracy: 0.3636\n",
            "Epoch 217/1000\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 218/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 219/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 220/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0119 - accuracy: 0.3636\n",
            "Epoch 221/1000\n",
            "9/9 [==============================] - 9s 947ms/step - loss: 0.0118 - accuracy: 0.3637\n",
            "Epoch 222/1000\n",
            "9/9 [==============================] - 9s 955ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 223/1000\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0117 - accuracy: 0.3638\n",
            "Epoch 224/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0117 - accuracy: 0.3642\n",
            "Epoch 225/1000\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0119 - accuracy: 0.3637\n",
            "Epoch 226/1000\n",
            "9/9 [==============================] - 9s 948ms/step - loss: 0.0119 - accuracy: 0.3638\n",
            "Epoch 227/1000\n",
            "9/9 [==============================] - 9s 956ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 228/1000\n",
            "9/9 [==============================] - 9s 941ms/step - loss: 0.0117 - accuracy: 0.3642\n",
            "Epoch 229/1000\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 230/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0116 - accuracy: 0.3643\n",
            "Epoch 231/1000\n",
            "9/9 [==============================] - 9s 939ms/step - loss: 0.0121 - accuracy: 0.3638\n",
            "Epoch 232/1000\n",
            "9/9 [==============================] - 9s 934ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 233/1000\n",
            "9/9 [==============================] - 9s 959ms/step - loss: 0.0120 - accuracy: 0.3639\n",
            "Epoch 234/1000\n",
            "9/9 [==============================] - 8s 937ms/step - loss: 0.0120 - accuracy: 0.3638\n",
            "Epoch 235/1000\n",
            "9/9 [==============================] - 8s 932ms/step - loss: 0.0120 - accuracy: 0.3639\n",
            "Epoch 236/1000\n",
            "9/9 [==============================] - 8s 937ms/step - loss: 0.0118 - accuracy: 0.3643\n",
            "Epoch 237/1000\n",
            "9/9 [==============================] - 9s 947ms/step - loss: 0.0117 - accuracy: 0.3644\n",
            "Epoch 238/1000\n",
            "9/9 [==============================] - 9s 970ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 239/1000\n",
            "9/9 [==============================] - 9s 967ms/step - loss: 0.0168 - accuracy: 0.3633\n",
            "Epoch 240/1000\n",
            "9/9 [==============================] - 9s 963ms/step - loss: 0.0139 - accuracy: 0.3633\n",
            "Epoch 241/1000\n",
            "9/9 [==============================] - 9s 993ms/step - loss: 0.0126 - accuracy: 0.3640\n",
            "Epoch 242/1000\n",
            "9/9 [==============================] - 9s 964ms/step - loss: 0.0126 - accuracy: 0.3640\n",
            "Epoch 243/1000\n",
            "9/9 [==============================] - 9s 969ms/step - loss: 0.0121 - accuracy: 0.3635\n",
            "Epoch 244/1000\n",
            "9/9 [==============================] - 9s 975ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 245/1000\n",
            "9/9 [==============================] - 9s 971ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 246/1000\n",
            "9/9 [==============================] - 9s 971ms/step - loss: 0.0117 - accuracy: 0.3640\n",
            "Epoch 247/1000\n",
            "9/9 [==============================] - 9s 964ms/step - loss: 0.0120 - accuracy: 0.3642\n",
            "Epoch 248/1000\n",
            "9/9 [==============================] - 9s 964ms/step - loss: 0.0122 - accuracy: 0.3635\n",
            "Epoch 249/1000\n",
            "9/9 [==============================] - 9s 962ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 250/1000\n",
            "9/9 [==============================] - 9s 964ms/step - loss: 0.0116 - accuracy: 0.3636\n",
            "Epoch 251/1000\n",
            "9/9 [==============================] - 9s 979ms/step - loss: 0.0118 - accuracy: 0.3644\n",
            "Epoch 252/1000\n",
            "9/9 [==============================] - 9s 970ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 253/1000\n",
            "9/9 [==============================] - 9s 966ms/step - loss: 0.0120 - accuracy: 0.3630\n",
            "Epoch 254/1000\n",
            "9/9 [==============================] - 8s 931ms/step - loss: 0.0118 - accuracy: 0.3642\n",
            "Epoch 255/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0136 - accuracy: 0.3639\n",
            "Epoch 256/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0169 - accuracy: 0.3632\n",
            "Epoch 257/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0135 - accuracy: 0.3639\n",
            "Epoch 258/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0119 - accuracy: 0.3640\n",
            "Epoch 259/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0119 - accuracy: 0.3641\n",
            "Epoch 260/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 261/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 262/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0118 - accuracy: 0.3637\n",
            "Epoch 263/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 264/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 265/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 266/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0117 - accuracy: 0.3642\n",
            "Epoch 267/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 268/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 269/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 270/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0116 - accuracy: 0.3642\n",
            "Epoch 271/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0117 - accuracy: 0.3638\n",
            "Epoch 272/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0117 - accuracy: 0.3635\n",
            "Epoch 273/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0119 - accuracy: 0.3638\n",
            "Epoch 274/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0119 - accuracy: 0.3642\n",
            "Epoch 275/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 276/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 277/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0117 - accuracy: 0.3643\n",
            "Epoch 278/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 279/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0119 - accuracy: 0.3642\n",
            "Epoch 280/1000\n",
            "9/9 [==============================] - 9s 944ms/step - loss: 0.0118 - accuracy: 0.3637\n",
            "Epoch 281/1000\n",
            "9/9 [==============================] - 8s 922ms/step - loss: 0.0119 - accuracy: 0.3638\n",
            "Epoch 282/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 283/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 284/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0118 - accuracy: 0.3636\n",
            "Epoch 285/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0119 - accuracy: 0.3637\n",
            "Epoch 286/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0120 - accuracy: 0.3637\n",
            "Epoch 287/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 288/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 289/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0116 - accuracy: 0.3642\n",
            "Epoch 290/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0120 - accuracy: 0.3642\n",
            "Epoch 291/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 292/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0118 - accuracy: 0.3643\n",
            "Epoch 293/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0130 - accuracy: 0.3636\n",
            "Epoch 294/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0128 - accuracy: 0.3638\n",
            "Epoch 295/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0145 - accuracy: 0.3634\n",
            "Epoch 296/1000\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0135 - accuracy: 0.3635\n",
            "Epoch 297/1000\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0141 - accuracy: 0.3639\n",
            "Epoch 298/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0127 - accuracy: 0.3641\n",
            "Epoch 299/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0119 - accuracy: 0.3641\n",
            "Epoch 300/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0120 - accuracy: 0.3638\n",
            "Epoch 301/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0121 - accuracy: 0.3636\n",
            "Epoch 302/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0116 - accuracy: 0.3646\n",
            "Epoch 303/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 304/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0117 - accuracy: 0.3637\n",
            "Epoch 305/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0117 - accuracy: 0.3636\n",
            "Epoch 306/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 307/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0115 - accuracy: 0.3637\n",
            "Epoch 308/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 309/1000\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 310/1000\n",
            "9/9 [==============================] - 9s 975ms/step - loss: 0.0117 - accuracy: 0.3637\n",
            "Epoch 311/1000\n",
            "9/9 [==============================] - 8s 894ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 312/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0117 - accuracy: 0.3636\n",
            "Epoch 313/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 314/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 315/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0120 - accuracy: 0.3634\n",
            "Epoch 316/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 317/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 318/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0117 - accuracy: 0.3642\n",
            "Epoch 319/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0116 - accuracy: 0.3643\n",
            "Epoch 320/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 321/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0116 - accuracy: 0.3643\n",
            "Epoch 322/1000\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 323/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0123 - accuracy: 0.3640\n",
            "Epoch 324/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 325/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0121 - accuracy: 0.3641\n",
            "Epoch 326/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 327/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 328/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 329/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0118 - accuracy: 0.3637\n",
            "Epoch 330/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0119 - accuracy: 0.3636\n",
            "Epoch 331/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 332/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 333/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 334/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0134 - accuracy: 0.3633\n",
            "Epoch 335/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0160 - accuracy: 0.3629\n",
            "Epoch 336/1000\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0135 - accuracy: 0.3636\n",
            "Epoch 337/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0124 - accuracy: 0.3638\n",
            "Epoch 338/1000\n",
            "9/9 [==============================] - 8s 937ms/step - loss: 0.0120 - accuracy: 0.3638\n",
            "Epoch 339/1000\n",
            "9/9 [==============================] - 8s 925ms/step - loss: 0.0117 - accuracy: 0.3638\n",
            "Epoch 340/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0120 - accuracy: 0.3639\n",
            "Epoch 341/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 342/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0129 - accuracy: 0.3636\n",
            "Epoch 343/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0156 - accuracy: 0.3631\n",
            "Epoch 344/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0132 - accuracy: 0.3634\n",
            "Epoch 345/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0124 - accuracy: 0.3639\n",
            "Epoch 346/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0122 - accuracy: 0.3635\n",
            "Epoch 347/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0125 - accuracy: 0.3641\n",
            "Epoch 348/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0122 - accuracy: 0.3641\n",
            "Epoch 349/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0117 - accuracy: 0.3640\n",
            "Epoch 350/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 351/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0119 - accuracy: 0.3638\n",
            "Epoch 352/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0130 - accuracy: 0.3640\n",
            "Epoch 353/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0123 - accuracy: 0.3638\n",
            "Epoch 354/1000\n",
            "9/9 [==============================] - 9s 966ms/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 355/1000\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 356/1000\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 357/1000\n",
            "9/9 [==============================] - 8s 879ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 358/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0119 - accuracy: 0.3638\n",
            "Epoch 359/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 360/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0117 - accuracy: 0.3642\n",
            "Epoch 361/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 362/1000\n",
            "9/9 [==============================] - 8s 874ms/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 363/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 364/1000\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0120 - accuracy: 0.3633\n",
            "Epoch 365/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 366/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 367/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0116 - accuracy: 0.3635\n",
            "Epoch 368/1000\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0119 - accuracy: 0.3644\n",
            "Epoch 369/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0117 - accuracy: 0.3638\n",
            "Epoch 370/1000\n",
            "9/9 [==============================] - 8s 925ms/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 371/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 372/1000\n",
            "9/9 [==============================] - 9s 973ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 373/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0118 - accuracy: 0.3637\n",
            "Epoch 374/1000\n",
            "9/9 [==============================] - 8s 877ms/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 375/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 376/1000\n",
            "9/9 [==============================] - 8s 878ms/step - loss: 0.0119 - accuracy: 0.3642\n",
            "Epoch 377/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0116 - accuracy: 0.3643\n",
            "Epoch 378/1000\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0116 - accuracy: 0.3636\n",
            "Epoch 379/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 380/1000\n",
            "9/9 [==============================] - 8s 882ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 381/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0118 - accuracy: 0.3636\n",
            "Epoch 382/1000\n",
            "9/9 [==============================] - 9s 954ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 383/1000\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 384/1000\n",
            "9/9 [==============================] - 8s 880ms/step - loss: 0.0116 - accuracy: 0.3643\n",
            "Epoch 385/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 386/1000\n",
            "9/9 [==============================] - 8s 888ms/step - loss: 0.0125 - accuracy: 0.3637\n",
            "Epoch 387/1000\n",
            "9/9 [==============================] - 8s 916ms/step - loss: 0.0121 - accuracy: 0.3640\n",
            "Epoch 388/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0125 - accuracy: 0.3636\n",
            "Epoch 389/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0142 - accuracy: 0.3635\n",
            "Epoch 390/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0127 - accuracy: 0.3637\n",
            "Epoch 391/1000\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0144 - accuracy: 0.3635\n",
            "Epoch 392/1000\n",
            "9/9 [==============================] - 8s 894ms/step - loss: 0.0124 - accuracy: 0.3636\n",
            "Epoch 393/1000\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.0125 - accuracy: 0.3640\n",
            "Epoch 394/1000\n",
            "9/9 [==============================] - 8s 878ms/step - loss: 0.0123 - accuracy: 0.3637\n",
            "Epoch 395/1000\n",
            "9/9 [==============================] - 8s 877ms/step - loss: 0.0117 - accuracy: 0.3640\n",
            "Epoch 396/1000\n",
            "9/9 [==============================] - 8s 925ms/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 397/1000\n",
            "9/9 [==============================] - 8s 922ms/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 398/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 399/1000\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 400/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0117 - accuracy: 0.3640\n",
            "Epoch 401/1000\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0118 - accuracy: 0.3636\n",
            "Epoch 402/1000\n",
            "9/9 [==============================] - 8s 882ms/step - loss: 0.0123 - accuracy: 0.3639\n",
            "Epoch 403/1000\n",
            "9/9 [==============================] - 8s 927ms/step - loss: 0.0122 - accuracy: 0.3637\n",
            "Epoch 404/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 405/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0132 - accuracy: 0.3639\n",
            "Epoch 406/1000\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0131 - accuracy: 0.3636\n",
            "Epoch 407/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0130 - accuracy: 0.3634\n",
            "Epoch 408/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0136 - accuracy: 0.3637\n",
            "Epoch 409/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0141 - accuracy: 0.3632\n",
            "Epoch 410/1000\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.0128 - accuracy: 0.3634\n",
            "Epoch 411/1000\n",
            "9/9 [==============================] - 8s 929ms/step - loss: 0.0119 - accuracy: 0.3641\n",
            "Epoch 412/1000\n",
            "9/9 [==============================] - 9s 934ms/step - loss: 0.0117 - accuracy: 0.3640\n",
            "Epoch 413/1000\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0117 - accuracy: 0.3644\n",
            "Epoch 414/1000\n",
            "9/9 [==============================] - 8s 870ms/step - loss: 0.0115 - accuracy: 0.3646\n",
            "Epoch 415/1000\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0116 - accuracy: 0.3644\n",
            "Epoch 416/1000\n",
            "9/9 [==============================] - 8s 863ms/step - loss: 0.0121 - accuracy: 0.3641\n",
            "Epoch 417/1000\n",
            "9/9 [==============================] - 8s 872ms/step - loss: 0.0119 - accuracy: 0.3636\n",
            "Epoch 418/1000\n",
            "9/9 [==============================] - 8s 863ms/step - loss: 0.0119 - accuracy: 0.3641\n",
            "Epoch 419/1000\n",
            "9/9 [==============================] - 8s 880ms/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 420/1000\n",
            "9/9 [==============================] - 8s 874ms/step - loss: 0.0117 - accuracy: 0.3640\n",
            "Epoch 421/1000\n",
            "9/9 [==============================] - 8s 869ms/step - loss: 0.0118 - accuracy: 0.3635\n",
            "Epoch 422/1000\n",
            "9/9 [==============================] - 8s 871ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 423/1000\n",
            "9/9 [==============================] - 8s 921ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 424/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 425/1000\n",
            "9/9 [==============================] - 8s 878ms/step - loss: 0.0115 - accuracy: 0.3644\n",
            "Epoch 426/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 427/1000\n",
            "9/9 [==============================] - 8s 894ms/step - loss: 0.0115 - accuracy: 0.3647\n",
            "Epoch 428/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0122 - accuracy: 0.3636\n",
            "Epoch 429/1000\n",
            "9/9 [==============================] - 8s 868ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 430/1000\n",
            "9/9 [==============================] - 8s 872ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 431/1000\n",
            "9/9 [==============================] - 8s 865ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 432/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 433/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 434/1000\n",
            "9/9 [==============================] - 9s 985ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 435/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0117 - accuracy: 0.3643\n",
            "Epoch 436/1000\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0115 - accuracy: 0.3638\n",
            "Epoch 437/1000\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0117 - accuracy: 0.3637\n",
            "Epoch 438/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 439/1000\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0115 - accuracy: 0.3644\n",
            "Epoch 440/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0120 - accuracy: 0.3637\n",
            "Epoch 441/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0117 - accuracy: 0.3640\n",
            "Epoch 442/1000\n",
            "9/9 [==============================] - 8s 877ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 443/1000\n",
            "9/9 [==============================] - 8s 877ms/step - loss: 0.0120 - accuracy: 0.3637\n",
            "Epoch 444/1000\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.0127 - accuracy: 0.3638\n",
            "Epoch 445/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0126 - accuracy: 0.3635\n",
            "Epoch 446/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0130 - accuracy: 0.3639\n",
            "Epoch 447/1000\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0134 - accuracy: 0.3639\n",
            "Epoch 448/1000\n",
            "9/9 [==============================] - 8s 880ms/step - loss: 0.0127 - accuracy: 0.3636\n",
            "Epoch 449/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0122 - accuracy: 0.3638\n",
            "Epoch 450/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0126 - accuracy: 0.3639\n",
            "Epoch 451/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0122 - accuracy: 0.3638\n",
            "Epoch 452/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0119 - accuracy: 0.3642\n",
            "Epoch 453/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0120 - accuracy: 0.3638\n",
            "Epoch 454/1000\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0122 - accuracy: 0.3636\n",
            "Epoch 455/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0123 - accuracy: 0.3638\n",
            "Epoch 456/1000\n",
            "9/9 [==============================] - 9s 966ms/step - loss: 0.0121 - accuracy: 0.3636\n",
            "Epoch 457/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0120 - accuracy: 0.3636\n",
            "Epoch 458/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0124 - accuracy: 0.3638\n",
            "Epoch 459/1000\n",
            "9/9 [==============================] - 8s 934ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 460/1000\n",
            "9/9 [==============================] - 9s 951ms/step - loss: 0.0121 - accuracy: 0.3640\n",
            "Epoch 461/1000\n",
            "9/9 [==============================] - 9s 934ms/step - loss: 0.0119 - accuracy: 0.3640\n",
            "Epoch 462/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0124 - accuracy: 0.3638\n",
            "Epoch 463/1000\n",
            "9/9 [==============================] - 8s 927ms/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 464/1000\n",
            "9/9 [==============================] - 8s 939ms/step - loss: 0.0116 - accuracy: 0.3642\n",
            "Epoch 465/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 466/1000\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 467/1000\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 468/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0119 - accuracy: 0.3634\n",
            "Epoch 469/1000\n",
            "9/9 [==============================] - 8s 878ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 470/1000\n",
            "9/9 [==============================] - 8s 878ms/step - loss: 0.0120 - accuracy: 0.3639\n",
            "Epoch 471/1000\n",
            "9/9 [==============================] - 8s 877ms/step - loss: 0.0116 - accuracy: 0.3643\n",
            "Epoch 472/1000\n",
            "9/9 [==============================] - 8s 878ms/step - loss: 0.0118 - accuracy: 0.3636\n",
            "Epoch 473/1000\n",
            "9/9 [==============================] - 8s 873ms/step - loss: 0.0116 - accuracy: 0.3639\n",
            "Epoch 474/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0116 - accuracy: 0.3642\n",
            "Epoch 475/1000\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0116 - accuracy: 0.3643\n",
            "Epoch 476/1000\n",
            "9/9 [==============================] - 8s 875ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 477/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 478/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0114 - accuracy: 0.3644\n",
            "Epoch 479/1000\n",
            "9/9 [==============================] - 8s 928ms/step - loss: 0.0116 - accuracy: 0.3644\n",
            "Epoch 480/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0117 - accuracy: 0.3638\n",
            "Epoch 481/1000\n",
            "9/9 [==============================] - 8s 888ms/step - loss: 0.0115 - accuracy: 0.3644\n",
            "Epoch 482/1000\n",
            "9/9 [==============================] - 8s 941ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 483/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 484/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 485/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0114 - accuracy: 0.3644\n",
            "Epoch 486/1000\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 487/1000\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0117 - accuracy: 0.3640\n",
            "Epoch 488/1000\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0117 - accuracy: 0.3638\n",
            "Epoch 489/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0117 - accuracy: 0.3642\n",
            "Epoch 490/1000\n",
            "9/9 [==============================] - 8s 876ms/step - loss: 0.0133 - accuracy: 0.3635\n",
            "Epoch 491/1000\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0122 - accuracy: 0.3643\n",
            "Epoch 492/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0124 - accuracy: 0.3640\n",
            "Epoch 493/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0126 - accuracy: 0.3638\n",
            "Epoch 494/1000\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.0128 - accuracy: 0.3637\n",
            "Epoch 495/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0139 - accuracy: 0.3632\n",
            "Epoch 496/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0134 - accuracy: 0.3636\n",
            "Epoch 497/1000\n",
            "9/9 [==============================] - 8s 923ms/step - loss: 0.0130 - accuracy: 0.3638\n",
            "Epoch 498/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0126 - accuracy: 0.3635\n",
            "Epoch 499/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0121 - accuracy: 0.3638\n",
            "Epoch 500/1000\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 501/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0117 - accuracy: 0.3642\n",
            "Epoch 502/1000\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0125 - accuracy: 0.3643\n",
            "Epoch 503/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0120 - accuracy: 0.3637\n",
            "Epoch 504/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0123 - accuracy: 0.3639\n",
            "Epoch 505/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0119 - accuracy: 0.3640\n",
            "Epoch 506/1000\n",
            "9/9 [==============================] - 8s 877ms/step - loss: 0.0125 - accuracy: 0.3641\n",
            "Epoch 507/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0130 - accuracy: 0.3638\n",
            "Epoch 508/1000\n",
            "9/9 [==============================] - 8s 879ms/step - loss: 0.0120 - accuracy: 0.3639\n",
            "Epoch 509/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0122 - accuracy: 0.3639\n",
            "Epoch 510/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 511/1000\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 512/1000\n",
            "9/9 [==============================] - 8s 894ms/step - loss: 0.0120 - accuracy: 0.3641\n",
            "Epoch 513/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 514/1000\n",
            "9/9 [==============================] - 8s 931ms/step - loss: 0.0116 - accuracy: 0.3643\n",
            "Epoch 515/1000\n",
            "9/9 [==============================] - 8s 929ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 516/1000\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 517/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 518/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0116 - accuracy: 0.3639\n",
            "Epoch 519/1000\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 520/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0116 - accuracy: 0.3636\n",
            "Epoch 521/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0117 - accuracy: 0.3638\n",
            "Epoch 522/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 523/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 524/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0115 - accuracy: 0.3636\n",
            "Epoch 525/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 526/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 527/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 528/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 529/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 530/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0116 - accuracy: 0.3642\n",
            "Epoch 531/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0117 - accuracy: 0.3640\n",
            "Epoch 532/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 533/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 534/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 535/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 536/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 537/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 538/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 539/1000\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0113 - accuracy: 0.3645\n",
            "Epoch 540/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0115 - accuracy: 0.3638\n",
            "Epoch 541/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 542/1000\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 543/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 544/1000\n",
            "9/9 [==============================] - 9s 951ms/step - loss: 0.0132 - accuracy: 0.3639\n",
            "Epoch 545/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0143 - accuracy: 0.3635\n",
            "Epoch 546/1000\n",
            "9/9 [==============================] - 8s 888ms/step - loss: 0.0129 - accuracy: 0.3637\n",
            "Epoch 547/1000\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0129 - accuracy: 0.3639\n",
            "Epoch 548/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0130 - accuracy: 0.3637\n",
            "Epoch 549/1000\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0119 - accuracy: 0.3641\n",
            "Epoch 550/1000\n",
            "9/9 [==============================] - 8s 879ms/step - loss: 0.0119 - accuracy: 0.3638\n",
            "Epoch 551/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0133 - accuracy: 0.3639\n",
            "Epoch 552/1000\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0121 - accuracy: 0.3641\n",
            "Epoch 553/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0119 - accuracy: 0.3642\n",
            "Epoch 554/1000\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0115 - accuracy: 0.3638\n",
            "Epoch 555/1000\n",
            "9/9 [==============================] - 8s 935ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 556/1000\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 557/1000\n",
            "9/9 [==============================] - 8s 878ms/step - loss: 0.0113 - accuracy: 0.3644\n",
            "Epoch 558/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0113 - accuracy: 0.3639\n",
            "Epoch 559/1000\n",
            "9/9 [==============================] - 8s 878ms/step - loss: 0.0114 - accuracy: 0.3646\n",
            "Epoch 560/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 561/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 562/1000\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 563/1000\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 564/1000\n",
            "9/9 [==============================] - 8s 876ms/step - loss: 0.0116 - accuracy: 0.3639\n",
            "Epoch 565/1000\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0141 - accuracy: 0.3635\n",
            "Epoch 566/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0121 - accuracy: 0.3638\n",
            "Epoch 567/1000\n",
            "9/9 [==============================] - 9s 945ms/step - loss: 0.0122 - accuracy: 0.3637\n",
            "Epoch 568/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0122 - accuracy: 0.3638\n",
            "Epoch 569/1000\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 570/1000\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 571/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0115 - accuracy: 0.3644\n",
            "Epoch 572/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 573/1000\n",
            "9/9 [==============================] - 8s 927ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 574/1000\n",
            "9/9 [==============================] - 8s 921ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 575/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 576/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0117 - accuracy: 0.3642\n",
            "Epoch 577/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0117 - accuracy: 0.3642\n",
            "Epoch 578/1000\n",
            "9/9 [==============================] - 8s 875ms/step - loss: 0.0115 - accuracy: 0.3645\n",
            "Epoch 579/1000\n",
            "9/9 [==============================] - 8s 878ms/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 580/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 581/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 582/1000\n",
            "9/9 [==============================] - 8s 880ms/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 583/1000\n",
            "9/9 [==============================] - 8s 879ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 584/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 585/1000\n",
            "9/9 [==============================] - 8s 888ms/step - loss: 0.0117 - accuracy: 0.3635\n",
            "Epoch 586/1000\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0119 - accuracy: 0.3638\n",
            "Epoch 587/1000\n",
            "9/9 [==============================] - 8s 878ms/step - loss: 0.0128 - accuracy: 0.3635\n",
            "Epoch 588/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0118 - accuracy: 0.3642\n",
            "Epoch 589/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0124 - accuracy: 0.3639\n",
            "Epoch 590/1000\n",
            "9/9 [==============================] - 8s 880ms/step - loss: 0.0122 - accuracy: 0.3641\n",
            "Epoch 591/1000\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0116 - accuracy: 0.3639\n",
            "Epoch 592/1000\n",
            "9/9 [==============================] - 8s 932ms/step - loss: 0.0116 - accuracy: 0.3642\n",
            "Epoch 593/1000\n",
            "9/9 [==============================] - 8s 875ms/step - loss: 0.0118 - accuracy: 0.3635\n",
            "Epoch 594/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0116 - accuracy: 0.3637\n",
            "Epoch 595/1000\n",
            "9/9 [==============================] - 8s 928ms/step - loss: 0.0113 - accuracy: 0.3641\n",
            "Epoch 596/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 597/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 598/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 599/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 600/1000\n",
            "9/9 [==============================] - 8s 888ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 601/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0113 - accuracy: 0.3643\n",
            "Epoch 602/1000\n",
            "9/9 [==============================] - 8s 939ms/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 603/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 604/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0114 - accuracy: 0.3644\n",
            "Epoch 605/1000\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 0.0114 - accuracy: 0.3637\n",
            "Epoch 606/1000\n",
            "9/9 [==============================] - 8s 923ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 607/1000\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0115 - accuracy: 0.3637\n",
            "Epoch 608/1000\n",
            "9/9 [==============================] - 8s 921ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 609/1000\n",
            "9/9 [==============================] - 8s 932ms/step - loss: 0.0116 - accuracy: 0.3637\n",
            "Epoch 610/1000\n",
            "9/9 [==============================] - 9s 959ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 611/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 612/1000\n",
            "9/9 [==============================] - 8s 877ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 613/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0113 - accuracy: 0.3644\n",
            "Epoch 614/1000\n",
            "9/9 [==============================] - 8s 879ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 615/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 616/1000\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 617/1000\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 618/1000\n",
            "9/9 [==============================] - 8s 882ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 619/1000\n",
            "9/9 [==============================] - 8s 879ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 620/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0116 - accuracy: 0.3642\n",
            "Epoch 621/1000\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.0121 - accuracy: 0.3639\n",
            "Epoch 622/1000\n",
            "9/9 [==============================] - 8s 878ms/step - loss: 0.0121 - accuracy: 0.3645\n",
            "Epoch 623/1000\n",
            "9/9 [==============================] - 8s 880ms/step - loss: 0.0121 - accuracy: 0.3642\n",
            "Epoch 624/1000\n",
            "9/9 [==============================] - 8s 874ms/step - loss: 0.0116 - accuracy: 0.3643\n",
            "Epoch 625/1000\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0127 - accuracy: 0.3638\n",
            "Epoch 626/1000\n",
            "9/9 [==============================] - 8s 880ms/step - loss: 0.0127 - accuracy: 0.3637\n",
            "Epoch 627/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0140 - accuracy: 0.3634\n",
            "Epoch 628/1000\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.0124 - accuracy: 0.3637\n",
            "Epoch 629/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0123 - accuracy: 0.3637\n",
            "Epoch 630/1000\n",
            "9/9 [==============================] - 8s 879ms/step - loss: 0.0120 - accuracy: 0.3639\n",
            "Epoch 631/1000\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 632/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0116 - accuracy: 0.3636\n",
            "Epoch 633/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 634/1000\n",
            "9/9 [==============================] - 8s 923ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 635/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 636/1000\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0115 - accuracy: 0.3636\n",
            "Epoch 637/1000\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 638/1000\n",
            "9/9 [==============================] - 8s 880ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 639/1000\n",
            "9/9 [==============================] - 8s 880ms/step - loss: 0.0114 - accuracy: 0.3645\n",
            "Epoch 640/1000\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 641/1000\n",
            "9/9 [==============================] - 8s 875ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 642/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0130 - accuracy: 0.3637\n",
            "Epoch 643/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0121 - accuracy: 0.3637\n",
            "Epoch 644/1000\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0122 - accuracy: 0.3644\n",
            "Epoch 645/1000\n",
            "9/9 [==============================] - 8s 876ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 646/1000\n",
            "9/9 [==============================] - 8s 888ms/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 647/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 648/1000\n",
            "9/9 [==============================] - 8s 880ms/step - loss: 0.0115 - accuracy: 0.3637\n",
            "Epoch 649/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 650/1000\n",
            "9/9 [==============================] - 8s 880ms/step - loss: 0.0114 - accuracy: 0.3637\n",
            "Epoch 651/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 652/1000\n",
            "9/9 [==============================] - 8s 873ms/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 653/1000\n",
            "9/9 [==============================] - 8s 879ms/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 654/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0139 - accuracy: 0.3634\n",
            "Epoch 655/1000\n",
            "9/9 [==============================] - 8s 873ms/step - loss: 0.0133 - accuracy: 0.3638\n",
            "Epoch 656/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0133 - accuracy: 0.3638\n",
            "Epoch 657/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0156 - accuracy: 0.3636\n",
            "Epoch 658/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0122 - accuracy: 0.3636\n",
            "Epoch 659/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0121 - accuracy: 0.3638\n",
            "Epoch 660/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0130 - accuracy: 0.3639\n",
            "Epoch 661/1000\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 662/1000\n",
            "9/9 [==============================] - 8s 925ms/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 663/1000\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 664/1000\n",
            "9/9 [==============================] - 8s 922ms/step - loss: 0.0116 - accuracy: 0.3642\n",
            "Epoch 665/1000\n",
            "9/9 [==============================] - 9s 994ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 666/1000\n",
            "9/9 [==============================] - 9s 950ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 667/1000\n",
            "9/9 [==============================] - 9s 941ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 668/1000\n",
            "9/9 [==============================] - 8s 927ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 669/1000\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 670/1000\n",
            "9/9 [==============================] - 8s 939ms/step - loss: 0.0114 - accuracy: 0.3644\n",
            "Epoch 671/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 672/1000\n",
            "9/9 [==============================] - 8s 924ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 673/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0115 - accuracy: 0.3644\n",
            "Epoch 674/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 675/1000\n",
            "9/9 [==============================] - 8s 930ms/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 676/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 677/1000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 678/1000\n",
            "9/9 [==============================] - 8s 934ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 679/1000\n",
            "9/9 [==============================] - 8s 924ms/step - loss: 0.0116 - accuracy: 0.3639\n",
            "Epoch 680/1000\n",
            "9/9 [==============================] - 9s 967ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 681/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 682/1000\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0114 - accuracy: 0.3638\n",
            "Epoch 683/1000\n",
            "9/9 [==============================] - 9s 956ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 684/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 685/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 686/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 687/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 688/1000\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0114 - accuracy: 0.3644\n",
            "Epoch 689/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 690/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 691/1000\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 692/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 693/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 694/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0114 - accuracy: 0.3636\n",
            "Epoch 695/1000\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 696/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0113 - accuracy: 0.3641\n",
            "Epoch 697/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0112 - accuracy: 0.3642\n",
            "Epoch 698/1000\n",
            "9/9 [==============================] - 8s 931ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 699/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0113 - accuracy: 0.3640\n",
            "Epoch 700/1000\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 701/1000\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0126 - accuracy: 0.3636\n",
            "Epoch 702/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 703/1000\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0114 - accuracy: 0.3646\n",
            "Epoch 704/1000\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 705/1000\n",
            "9/9 [==============================] - 9s 935ms/step - loss: 0.0115 - accuracy: 0.3637\n",
            "Epoch 706/1000\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 707/1000\n",
            "9/9 [==============================] - 8s 924ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 708/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 709/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0113 - accuracy: 0.3640\n",
            "Epoch 710/1000\n",
            "9/9 [==============================] - 8s 894ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 711/1000\n",
            "9/9 [==============================] - 8s 874ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 712/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0114 - accuracy: 0.3638\n",
            "Epoch 713/1000\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 714/1000\n",
            "9/9 [==============================] - 8s 869ms/step - loss: 0.0117 - accuracy: 0.3638\n",
            "Epoch 715/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0115 - accuracy: 0.3637\n",
            "Epoch 716/1000\n",
            "9/9 [==============================] - 8s 874ms/step - loss: 0.0122 - accuracy: 0.3639\n",
            "Epoch 717/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 718/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0118 - accuracy: 0.3637\n",
            "Epoch 719/1000\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0119 - accuracy: 0.3646\n",
            "Epoch 720/1000\n",
            "9/9 [==============================] - 8s 922ms/step - loss: 0.0121 - accuracy: 0.3641\n",
            "Epoch 721/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 722/1000\n",
            "9/9 [==============================] - 8s 875ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 723/1000\n",
            "9/9 [==============================] - 8s 874ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 724/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0117 - accuracy: 0.3637\n",
            "Epoch 725/1000\n",
            "9/9 [==============================] - 8s 870ms/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 726/1000\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0118 - accuracy: 0.3643\n",
            "Epoch 727/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0123 - accuracy: 0.3643\n",
            "Epoch 728/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0117 - accuracy: 0.3638\n",
            "Epoch 729/1000\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 730/1000\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0124 - accuracy: 0.3638\n",
            "Epoch 731/1000\n",
            "9/9 [==============================] - 8s 929ms/step - loss: 0.0135 - accuracy: 0.3638\n",
            "Epoch 732/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 733/1000\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 734/1000\n",
            "9/9 [==============================] - 8s 922ms/step - loss: 0.0119 - accuracy: 0.3646\n",
            "Epoch 735/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 736/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0114 - accuracy: 0.3637\n",
            "Epoch 737/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0121 - accuracy: 0.3640\n",
            "Epoch 738/1000\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0120 - accuracy: 0.3637\n",
            "Epoch 739/1000\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.0123 - accuracy: 0.3639\n",
            "Epoch 740/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0125 - accuracy: 0.3640\n",
            "Epoch 741/1000\n",
            "9/9 [==============================] - 8s 923ms/step - loss: 0.0121 - accuracy: 0.3638\n",
            "Epoch 742/1000\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 743/1000\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0117 - accuracy: 0.3643\n",
            "Epoch 744/1000\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 745/1000\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0115 - accuracy: 0.3638\n",
            "Epoch 746/1000\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 747/1000\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 748/1000\n",
            "9/9 [==============================] - 8s 928ms/step - loss: 0.0114 - accuracy: 0.3635\n",
            "Epoch 749/1000\n",
            "9/9 [==============================] - 8s 888ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 750/1000\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 751/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0114 - accuracy: 0.3638\n",
            "Epoch 752/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0114 - accuracy: 0.3645\n",
            "Epoch 753/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 754/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0113 - accuracy: 0.3640\n",
            "Epoch 755/1000\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 756/1000\n",
            "9/9 [==============================] - 8s 894ms/step - loss: 0.0114 - accuracy: 0.3644\n",
            "Epoch 757/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 758/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 759/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 760/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 761/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0121 - accuracy: 0.3637\n",
            "Epoch 762/1000\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0122 - accuracy: 0.3638\n",
            "Epoch 763/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 764/1000\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 765/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 766/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 767/1000\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0114 - accuracy: 0.3645\n",
            "Epoch 768/1000\n",
            "9/9 [==============================] - 9s 956ms/step - loss: 0.0113 - accuracy: 0.3643\n",
            "Epoch 769/1000\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0129 - accuracy: 0.3643\n",
            "Epoch 770/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0121 - accuracy: 0.3636\n",
            "Epoch 771/1000\n",
            "9/9 [==============================] - 9s 955ms/step - loss: 0.0121 - accuracy: 0.3642\n",
            "Epoch 772/1000\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0126 - accuracy: 0.3635\n",
            "Epoch 773/1000\n",
            "9/9 [==============================] - 8s 902ms/step - loss: 0.0119 - accuracy: 0.3640\n",
            "Epoch 774/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0115 - accuracy: 0.3644\n",
            "Epoch 775/1000\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 776/1000\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 777/1000\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 778/1000\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0117 - accuracy: 0.3636\n",
            "Epoch 779/1000\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0114 - accuracy: 0.3644\n",
            "Epoch 780/1000\n",
            "9/9 [==============================] - 8s 923ms/step - loss: 0.0115 - accuracy: 0.3638\n",
            "Epoch 781/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0116 - accuracy: 0.3637\n",
            "Epoch 782/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0113 - accuracy: 0.3644\n",
            "Epoch 783/1000\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0116 - accuracy: 0.3636\n",
            "Epoch 784/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0113 - accuracy: 0.3640\n",
            "Epoch 785/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 786/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0125 - accuracy: 0.3635\n",
            "Epoch 787/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0120 - accuracy: 0.3640\n",
            "Epoch 788/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0116 - accuracy: 0.3642\n",
            "Epoch 789/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0148 - accuracy: 0.3640\n",
            "Epoch 790/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 791/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 792/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0123 - accuracy: 0.3640\n",
            "Epoch 793/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0118 - accuracy: 0.3640\n",
            "Epoch 794/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0117 - accuracy: 0.3637\n",
            "Epoch 795/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 796/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 797/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 798/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 799/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 800/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3643\n",
            "Epoch 801/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 802/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 803/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3646\n",
            "Epoch 804/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3645\n",
            "Epoch 805/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 806/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0115 - accuracy: 0.3635\n",
            "Epoch 807/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 808/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3637\n",
            "Epoch 809/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 810/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 811/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0121 - accuracy: 0.3640\n",
            "Epoch 812/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 813/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0117 - accuracy: 0.3639\n",
            "Epoch 814/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0119 - accuracy: 0.3643\n",
            "Epoch 815/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0132 - accuracy: 0.3635\n",
            "Epoch 816/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0126 - accuracy: 0.3641\n",
            "Epoch 817/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 818/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0121 - accuracy: 0.3639\n",
            "Epoch 819/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0115 - accuracy: 0.3645\n",
            "Epoch 820/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0116 - accuracy: 0.3639\n",
            "Epoch 821/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 822/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 823/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 824/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 825/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 826/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 827/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 828/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 829/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3637\n",
            "Epoch 830/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3644\n",
            "Epoch 831/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 832/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 833/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0116 - accuracy: 0.3642\n",
            "Epoch 834/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 835/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0121 - accuracy: 0.3639\n",
            "Epoch 836/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 837/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 838/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 839/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 840/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0116 - accuracy: 0.3645\n",
            "Epoch 841/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 842/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0117 - accuracy: 0.3640\n",
            "Epoch 843/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 844/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 845/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0119 - accuracy: 0.3642\n",
            "Epoch 846/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 847/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0115 - accuracy: 0.3638\n",
            "Epoch 848/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3645\n",
            "Epoch 849/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 850/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 851/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 852/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 853/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3637\n",
            "Epoch 854/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3641\n",
            "Epoch 855/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3639\n",
            "Epoch 856/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0113 - accuracy: 0.3639\n",
            "Epoch 857/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0113 - accuracy: 0.3639\n",
            "Epoch 858/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 859/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0113 - accuracy: 0.3644\n",
            "Epoch 860/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 861/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 862/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 863/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 864/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0115 - accuracy: 0.3637\n",
            "Epoch 865/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 866/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 867/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 868/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3643\n",
            "Epoch 869/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 870/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3646\n",
            "Epoch 871/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3644\n",
            "Epoch 872/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3638\n",
            "Epoch 873/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0112 - accuracy: 0.3646\n",
            "Epoch 874/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 875/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 876/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 877/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 878/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3640\n",
            "Epoch 879/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3645\n",
            "Epoch 880/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0116 - accuracy: 0.3644\n",
            "Epoch 881/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 882/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0122 - accuracy: 0.3639\n",
            "Epoch 883/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0123 - accuracy: 0.3638\n",
            "Epoch 884/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0137 - accuracy: 0.3639\n",
            "Epoch 885/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0120 - accuracy: 0.3638\n",
            "Epoch 886/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0115 - accuracy: 0.3641\n",
            "Epoch 887/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0113 - accuracy: 0.3645\n",
            "Epoch 888/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0117 - accuracy: 0.3641\n",
            "Epoch 889/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 890/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 891/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 892/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 893/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 894/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 895/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3643\n",
            "Epoch 896/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0113 - accuracy: 0.3643\n",
            "Epoch 897/1000\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 898/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0118 - accuracy: 0.3641\n",
            "Epoch 899/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0119 - accuracy: 0.3638\n",
            "Epoch 900/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 901/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 902/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0113 - accuracy: 0.3643\n",
            "Epoch 903/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0119 - accuracy: 0.3637\n",
            "Epoch 904/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0113 - accuracy: 0.3638\n",
            "Epoch 905/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 906/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0112 - accuracy: 0.3644\n",
            "Epoch 907/1000\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 908/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0114 - accuracy: 0.3638\n",
            "Epoch 909/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 910/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3646\n",
            "Epoch 911/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3640\n",
            "Epoch 912/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 913/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3643\n",
            "Epoch 914/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3644\n",
            "Epoch 915/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 916/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3635\n",
            "Epoch 917/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 918/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0112 - accuracy: 0.3644\n",
            "Epoch 919/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0113 - accuracy: 0.3637\n",
            "Epoch 920/1000\n",
            "9/9 [==============================] - 14s 2s/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 921/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 922/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 923/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 924/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3644\n",
            "Epoch 925/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 926/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0113 - accuracy: 0.3640\n",
            "Epoch 927/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 928/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3644\n",
            "Epoch 929/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 930/1000\n",
            "9/9 [==============================] - 9s 998ms/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 931/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 932/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0115 - accuracy: 0.3643\n",
            "Epoch 933/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 934/1000\n",
            "9/9 [==============================] - 9s 993ms/step - loss: 0.0116 - accuracy: 0.3639\n",
            "Epoch 935/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0115 - accuracy: 0.3640\n",
            "Epoch 936/1000\n",
            "9/9 [==============================] - 9s 993ms/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 937/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3640\n",
            "Epoch 938/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3638\n",
            "Epoch 939/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3641\n",
            "Epoch 940/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3645\n",
            "Epoch 941/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 942/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0113 - accuracy: 0.3641\n",
            "Epoch 943/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3640\n",
            "Epoch 944/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3644\n",
            "Epoch 945/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 946/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3645\n",
            "Epoch 947/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0115 - accuracy: 0.3637\n",
            "Epoch 948/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0119 - accuracy: 0.3638\n",
            "Epoch 949/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0121 - accuracy: 0.3643\n",
            "Epoch 950/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0115 - accuracy: 0.3642\n",
            "Epoch 951/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 952/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3641\n",
            "Epoch 953/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0112 - accuracy: 0.3641\n",
            "Epoch 954/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0115 - accuracy: 0.3637\n",
            "Epoch 955/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3640\n",
            "Epoch 956/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3644\n",
            "Epoch 957/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0113 - accuracy: 0.3638\n",
            "Epoch 958/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0112 - accuracy: 0.3644\n",
            "Epoch 959/1000\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0112 - accuracy: 0.3642\n",
            "Epoch 960/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0113 - accuracy: 0.3643\n",
            "Epoch 961/1000\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0114 - accuracy: 0.3638\n",
            "Epoch 962/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 963/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0112 - accuracy: 0.3645\n",
            "Epoch 964/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 965/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3635\n",
            "Epoch 966/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3641\n",
            "Epoch 967/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 968/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3646\n",
            "Epoch 969/1000\n",
            "9/9 [==============================] - 9s 986ms/step - loss: 0.0113 - accuracy: 0.3642\n",
            "Epoch 970/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3641\n",
            "Epoch 971/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0113 - accuracy: 0.3640\n",
            "Epoch 972/1000\n",
            "9/9 [==============================] - 9s 967ms/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 973/1000\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0115 - accuracy: 0.3636\n",
            "Epoch 974/1000\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0113 - accuracy: 0.3641\n",
            "Epoch 975/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0118 - accuracy: 0.3644\n",
            "Epoch 976/1000\n",
            "9/9 [==============================] - 9s 978ms/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 977/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0118 - accuracy: 0.3639\n",
            "Epoch 978/1000\n",
            "9/9 [==============================] - 9s 960ms/step - loss: 0.0120 - accuracy: 0.3634\n",
            "Epoch 979/1000\n",
            "9/9 [==============================] - 9s 954ms/step - loss: 0.0116 - accuracy: 0.3637\n",
            "Epoch 980/1000\n",
            "9/9 [==============================] - 9s 966ms/step - loss: 0.0114 - accuracy: 0.3644\n",
            "Epoch 981/1000\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0115 - accuracy: 0.3635\n",
            "Epoch 982/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0116 - accuracy: 0.3638\n",
            "Epoch 983/1000\n",
            "9/9 [==============================] - 9s 971ms/step - loss: 0.0116 - accuracy: 0.3640\n",
            "Epoch 984/1000\n",
            "9/9 [==============================] - 9s 955ms/step - loss: 0.0118 - accuracy: 0.3638\n",
            "Epoch 985/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0113 - accuracy: 0.3644\n",
            "Epoch 986/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 987/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0112 - accuracy: 0.3642\n",
            "Epoch 988/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0112 - accuracy: 0.3644\n",
            "Epoch 989/1000\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0114 - accuracy: 0.3639\n",
            "Epoch 990/1000\n",
            "9/9 [==============================] - 14s 1s/step - loss: 0.0114 - accuracy: 0.3643\n",
            "Epoch 991/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0119 - accuracy: 0.3639\n",
            "Epoch 992/1000\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0118 - accuracy: 0.3642\n",
            "Epoch 993/1000\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 994/1000\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0117 - accuracy: 0.3644\n",
            "Epoch 995/1000\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0114 - accuracy: 0.3641\n",
            "Epoch 996/1000\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0115 - accuracy: 0.3639\n",
            "Epoch 997/1000\n",
            "9/9 [==============================] - 9s 954ms/step - loss: 0.0114 - accuracy: 0.3642\n",
            "Epoch 998/1000\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0113 - accuracy: 0.3640\n",
            "Epoch 999/1000\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0116 - accuracy: 0.3641\n",
            "Epoch 1000/1000\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0113 - accuracy: 0.3641\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x24769c5c430>"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(dataset, epochs=EPOCHS)\n",
        "# model.fit(dataset, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRsSO_I3NdRt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### --- Save and load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDd64uI9NdRt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: chatbot_campus_transformer_model\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: chatbot_campus_transformer_model\\assets\n"
          ]
        }
      ],
      "source": [
        "# save model\n",
        "model.save(\"chatbot_campus_transformer_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save tokenizer\n",
        "tokenizer.save_to_file(\"chatbot_campus_transformer_tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "W0fspHMnZPJl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndel model\\ntf.keras.backend.clear_session()\\n'"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "del model\n",
        "tf.keras.backend.clear_session()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1DUXog6WqV-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## -- Evaluate and predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### --- Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "_NjsS3zuAbRn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: Hi, there!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A: <a target=\"\\&undscblank\" href=\"$\\&undscmap\\&undscurl\"> here</a>\n",
            "-------------------------------------------\n",
            "Q: tell me about admission!\n",
            "A: application can also be submitted online through the unversity's <a target=\"\\&undscblank\" href=\"$\\&undscadmission\\&undscurl\">website</a>\n"
          ]
        }
      ],
      "source": [
        "def evaluate(sentence):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    sentence = tf.expand_dims(\n",
        "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0\n",
        "    )\n",
        "\n",
        "    output = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        predictions = model(inputs=[sentence, output], training=False)\n",
        "\n",
        "        # select the last word from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "            break\n",
        "\n",
        "        # concatenated the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0)\n",
        "\n",
        "\n",
        "def predict(sentence):\n",
        "    prediction = evaluate(sentence)\n",
        "    predicted_sentence = tokenizer.decode(\n",
        "        [i for i in prediction if i < tokenizer.vocab_size]\n",
        "    )\n",
        "    return predicted_sentence\n",
        "\n",
        "question = \"Hi, there!\"\n",
        "print(f\"Q: {question}\");\n",
        "print(f\"A: {predict(question)}\")\n",
        "print(\"-------------------------------------------\")\n",
        "\n",
        "question = \"tell me about admission!\"\n",
        "print(f\"Q: {question}\");\n",
        "print(f\"A: {predict(question)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sample_learning_rate = CustomSchedule(d_model=128)\n",
        "\n",
        "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### --- Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ---- Prepare Evaluation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19b6c6cf74a342608cd8f0a2c5856bd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating responses:   0%|          | 0/405 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Counts:\n",
            "- Questions: 405\n",
            "- Answers: 405\n",
            "- Bot Outputs: 405\n",
            "\n",
            "Sample:\n",
            "- Questions: ['hi', 'hi', 'how are you ?', 'is anyone there ?', 'hello', 'good day', \"what 's up\", 'how are ya', 'heyy', 'whatsup']\n",
            "- Answers: ['Hi there, how can I help?', 'Hello!', 'Hello!', 'Hello!', 'Hello, $_user!', 'Hi there, how can I help?', 'Hi there, how can I help?', 'Hi there, how can I help?', 'Hello, $_user!', 'Good to see you again!']\n",
            "- Bot Outputs: ['hello, $\\\\&undscuser!', 'hello, $\\\\&undscuser!', 'hello, $\\\\&undscuser!', 'hello, $\\\\&undscuser!', 'hello, $\\\\&undscuser!', 'hello, $\\\\&undscuser!', 'we serve franky, locho, alu-puri, kachori, khavsa, thaali and many more on menu', 'hello, $\\\\&undscuser!', 'hello, $\\\\&undscuser!', 'hello, $\\\\&undscuser!']\n",
            "\n",
            "All questions processed successfully\n"
          ]
        }
      ],
      "source": [
        "# Load the intent JSON\n",
        "with open(\"intents.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# load data\n",
        "with open(\"intents.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    intents_data = json.load(f)\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "for intent in intents_data[\"intents\"]:\n",
        "    for pattern in intent.get(\"patterns\", []):\n",
        "        questions.append(pattern)\n",
        "        answers.append(random.choice(intent[\"responses\"]))\n",
        "\n",
        "# preprocess\n",
        "processed_questions = [preprocess_text_per_word(q) for q in questions]\n",
        "processed_questions = [apply_ner_tags(text) for text in processed_questions]\n",
        "\n",
        "valid_questions = []\n",
        "valid_indices = []\n",
        "for i, q in enumerate(processed_questions):\n",
        "    if q.strip():\n",
        "        valid_questions.append(apply_ner_tags(q))\n",
        "        valid_indices.append(i)\n",
        "valid_answers = [answers[i] for i in valid_indices]\n",
        "\n",
        "# populate bot outputs\n",
        "bot_outputs = []\n",
        "# preprocessed_questions_eval_embed = np.array([get_bert_embedding(sentence, tokenizer, bert_model) for sentence in tqdm(preprocessed_questions_eval)])\n",
        "for i, question in enumerate(tqdm(valid_questions, desc=\"Generating responses\")):\n",
        "    answer = predict(question)\n",
        "    bot_outputs.append(answer)\n",
        " \n",
        "print(\"\\nFinal Counts:\")\n",
        "print(f\"- Questions: {len(valid_questions)}\")\n",
        "print(f\"- Answers: {len(valid_answers)}\")\n",
        "print(f\"- Bot Outputs: {len(bot_outputs)}\")\n",
        "\n",
        "print(\"\\nSample:\")\n",
        "print(f\"- Questions: {valid_questions[:10]}\")\n",
        "print(f\"- Answers: {valid_answers[:10]}\")\n",
        "print(f\"- Bot Outputs: {bot_outputs[:10]}\")\n",
        "\n",
        "\n",
        "if len(valid_questions) == len(bot_outputs):\n",
        "    print(\"\\nAll questions processed successfully\")\n",
        "else:\n",
        "    print(\"\\nMismatch in input/output counts!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ---- Rouge Score Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE-1: 0.8303\n",
            "ROUGE-2: 0.7351\n",
            "ROUGE-L: 0.8270\n"
          ]
        }
      ],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Evaluate all\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougeL_scores = []\n",
        "\n",
        "for ref, hyp in zip(valid_answers, bot_outputs):\n",
        "    scores = scorer.score(ref, hyp)\n",
        "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "# Average scores\n",
        "avg_r1 = sum(rouge1_scores) / len(rouge1_scores)\n",
        "avg_r2 = sum(rouge2_scores) / len(rouge2_scores)\n",
        "avg_rL = sum(rougeL_scores) / len(rougeL_scores)\n",
        "\n",
        "print(f\"ROUGE-1: {avg_r1:.4f}\")\n",
        "print(f\"ROUGE-2: {avg_r2:.4f}\")\n",
        "print(f\"ROUGE-L: {avg_rL:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ---- Bert Score Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf3e44695b894e80b20debbbbe06c54c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c868b624a1cc48429bd9122142e837e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 25.82 seconds, 15.69 sentences/sec\n",
            "\n",
            "BERTScore:\n",
            "Precision: 0.9233\n",
            "Recall:    0.9464\n",
            "F1 Score:  0.9344\n"
          ]
        }
      ],
      "source": [
        "from bert_score import score\n",
        "\n",
        "# Run BERTScore\n",
        "P, R, F1 = score(bot_outputs, valid_answers, lang=\"en\", verbose=True)\n",
        "\n",
        "# Average scores\n",
        "avg_precision = P.mean().item()\n",
        "avg_recall = R.mean().item()\n",
        "avg_f1 = F1.mean().item()\n",
        "\n",
        "print(f\"\\nBERTScore:\")\n",
        "print(f\"Precision: {avg_precision:.4f}\")\n",
        "print(f\"Recall:    {avg_recall:.4f}\")\n",
        "print(f\"F1 Score:  {avg_f1:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
