{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# NLP-related imports\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import spacy\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Masking\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 406\n",
      "Number of labels: 406\n",
      "Unique labels: {'committee', 'hours', 'creator', 'hostel', 'ithod', 'event', 'admission', 'name', 'sports', 'canteen', 'sem', 'salutaion', 'document', 'random', 'task', 'hod', 'infrastructure', 'course', 'menu', 'greeting', 'number', 'uniform', 'location', 'goodbye', 'scholarship', 'floors', 'vacation', 'fees', 'syllabus', 'library', 'college intake', 'swear', 'principal', 'extchod', 'computerhod', 'facilities', 'ragging', 'placement'}\n",
      "Sample texts: ['Hi', 'Hi', 'How are you?', 'Is anyone there?', 'Hello']\n",
      "Sample labels: ['greeting', 'greeting', 'greeting', 'greeting', 'greeting']\n"
     ]
    }
   ],
   "source": [
    "# Load intents\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare data\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        texts.append(pattern)\n",
    "        labels.append(intent['tag'])\n",
    "        \n",
    "# Summary\n",
    "print(f\"Number of texts: {len(texts)}\")\n",
    "print(f\"Number of labels: {len(labels)}\")\n",
    "print(f\"Unique labels: {set(labels)}\")\n",
    "print(f\"Sample texts: {texts[:5]}\")\n",
    "print(f\"Sample labels: {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Common Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== data summary =====\n",
      "Original number of texts: 406\n",
      "Number of texts after preprocessing: 405\n",
      "Number of skipped texts: 1\n",
      "Sample before preprocessing: ['Hi', 'Hi', 'How are you?', 'Is anyone there?', 'Hello']\n",
      "Sample after preprocessing: ['hi', 'hi', 'how are you?', 'is anyone there?', 'hello']\n",
      "\n",
      "===== label summary =====\n",
      "Original number of labels: 406\n",
      "Number of labels after preprocessing: 405\n",
      "Sample labels before preprocessing: ['greeting', 'greeting', 'greeting', 'greeting', 'greeting']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_text_per_word(text):\n",
    "    words = text.split()\n",
    "    processed_words = [word.lower() for word in words]\n",
    "    processed_words = [word.strip() for word in processed_words]\n",
    "    processed_words = [re.sub('\\s+',' ', word) for word in processed_words]\n",
    "    processed_words = [word for word in processed_words if not all(char in string.punctuation for char in word.replace(' ',''))]\n",
    "    \n",
    "    processed_words = ' '.join(processed_words)\n",
    "    return processed_words\n",
    "\n",
    "# Preprocess texts\n",
    "preprocessed_texts = [preprocess_text_per_word(text) for text in texts]\n",
    "\n",
    "filtered_texts = []\n",
    "skipped_indices = []\n",
    "for i, word in enumerate(preprocessed_texts):\n",
    "    if word.strip() != '':\n",
    "        filtered_texts.append(word)\n",
    "    else:\n",
    "        skipped_indices.append(i)\n",
    "        \n",
    "preprocessed_texts = filtered_texts\n",
    "labels = [label for i, label in enumerate(labels) if i not in skipped_indices]\n",
    "\n",
    "\n",
    "# summary\n",
    "print(\"===== data summary =====\")\n",
    "print(f\"Original number of texts: {len(texts)}\")\n",
    "print(f\"Number of texts after preprocessing: {len(preprocessed_texts)}\")\n",
    "print(f\"Number of skipped texts: {len(skipped_indices)}\")\n",
    "print(\"Sample before preprocessing:\", texts[:5])\n",
    "print(\"Sample after preprocessing:\", preprocessed_texts[:5])\n",
    "print()\n",
    "print(\"===== label summary =====\")\n",
    "print(f\"Original number of labels: {len(labels) + len(skipped_indices)}\")\n",
    "print(f\"Number of labels after preprocessing: {len(labels)}\")\n",
    "print(\"Sample labels before preprocessing:\", labels[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Semantic Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct NER tags: {'ORG', 'PERSON', 'DATE', 'ORDINAL', 'TIME'}\n",
      "Sample tagged texts: ['hi', 'hi', 'how are you ?', 'is anyone there ?', 'hello']\n",
      "Length of tagged texts: 405\n",
      "Length of distinct tags: 5\n"
     ]
    }
   ],
   "source": [
    "distinct_tags = set()\n",
    "\n",
    "def apply_ner_tags(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        distinct_tags.add(ent.label_)\n",
    "        \n",
    "    tagged_tokens = []\n",
    "    for token in doc:\n",
    "        replaced = False\n",
    "        for ent in doc.ents:\n",
    "            if token.text == ent.text:\n",
    "                tagged_tokens.append(f\"<{ent.label_}>\")\n",
    "                replaced = True\n",
    "                break\n",
    "        if not replaced:\n",
    "            tagged_tokens.append(token.text)\n",
    "    return \" \".join(tagged_tokens)\n",
    "\n",
    "tagged_texts = [apply_ner_tags(text) for text in preprocessed_texts]\n",
    "\n",
    "# Summary of distinct tags and tagged texts\n",
    "print(f\"Distinct NER tags: {distinct_tags}\")\n",
    "print(f\"Sample tagged texts: {tagged_texts[:5]}\")\n",
    "print(f\"Length of tagged texts: {len(tagged_texts)}\")\n",
    "print(f\"Length of distinct tags: {len(distinct_tags)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<DATE>', '<ORDINAL>', '<ORG>', '<PERSON>', '<TIME>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 405/405 [01:01<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of BERT embeddings array: (405, 30, 768) (Length: 405)\n",
      "Data type of BERT embeddings: float32\n",
      "Sample BERT embedding for the first text: [[-0.12822832  0.2022242  -0.04041219 ... -0.11584646  0.14928633\n",
      "   0.1449424 ]\n",
      " [-0.5613979  -0.17877388  0.23751609 ...  0.35251376  0.14657634\n",
      "  -0.46803072]\n",
      " [ 0.7654204   0.08086837 -0.2742688  ...  0.21145517 -0.67852986\n",
      "  -0.33935976]\n",
      " ...\n",
      " [-0.38684064  0.08147977  0.5432662  ...  0.00216477  0.11338539\n",
      "   0.14539245]\n",
      " [-0.5338742  -0.10825475  0.32787177 ...  0.14547752  0.18982382\n",
      "   0.14332972]\n",
      " [-0.42385688  0.00591564  0.49767217 ... -0.06936394  0.0515826\n",
      "   0.06124616]]\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embedding(sentence, tokenizer, model, max_len=30):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_len)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.squeeze(0).numpy()\n",
    "\n",
    "# add special tokens\n",
    "# Add angle brackets to each token\n",
    "special_tokens = [f\"<{token}>\" for token in distinct_tags]\n",
    "special_tokens = sorted(special_tokens)\n",
    "print(special_tokens)\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.resize_token_embeddings(len(tokenizer))\n",
    "bert_model.eval()  # disable gradient updates for inference\n",
    "\n",
    "X = np.array([get_bert_embedding(sentence, tokenizer, bert_model) for sentence in tqdm(tagged_texts)])\n",
    "\n",
    "# Summary of BERT embeddings\n",
    "print(f\"Shape of BERT embeddings array: {X.shape} (Length: {len(X)})\")\n",
    "print(f\"Data type of BERT embeddings: {X.dtype}\")\n",
    "print(f\"Sample BERT embedding for the first text: {X[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (405, 38)\n",
      "y data type: float32\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "y = to_categorical(encoded_labels)  # final label array\n",
    "\n",
    "# summary\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"y data type:\", y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Programs\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_1 (Masking)         (None, 30, 768)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 30, 512)           2623488   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30, 256)           787456    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               66048     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 38)                4902      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3843238 (14.66 MB)\n",
      "Trainable params: 3843238 (14.66 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# masking layer\n",
    "model.add(Masking(mask_value=0., input_shape=(X_train.shape[1], X_train.shape[2])))  # (max_len, 768)\n",
    "\n",
    "# lstm layer\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "\n",
    "# additional layers\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From d:\\Programs\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Programs\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "9/9 [==============================] - 306s 2s/step - loss: 3.5514 - accuracy: 0.1120 - val_loss: 3.2535 - val_accuracy: 0.2000\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 5s 522ms/step - loss: 2.9759 - accuracy: 0.2201 - val_loss: 2.7705 - val_accuracy: 0.2769\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 2.2820 - accuracy: 0.4208 - val_loss: 2.4346 - val_accuracy: 0.2769\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 4s 488ms/step - loss: 1.6891 - accuracy: 0.5753 - val_loss: 2.2685 - val_accuracy: 0.4462\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 4s 486ms/step - loss: 1.1443 - accuracy: 0.6795 - val_loss: 2.9072 - val_accuracy: 0.3538\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 4s 499ms/step - loss: 1.0952 - accuracy: 0.6911 - val_loss: 2.3928 - val_accuracy: 0.4615\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 4s 492ms/step - loss: 0.9261 - accuracy: 0.7529 - val_loss: 2.1317 - val_accuracy: 0.5077\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 4s 484ms/step - loss: 0.9961 - accuracy: 0.7104 - val_loss: 2.4134 - val_accuracy: 0.4769\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 4s 502ms/step - loss: 0.8936 - accuracy: 0.7606 - val_loss: 2.4598 - val_accuracy: 0.5077\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 4s 492ms/step - loss: 0.5511 - accuracy: 0.8494 - val_loss: 2.6051 - val_accuracy: 0.4769\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 4s 480ms/step - loss: 0.4378 - accuracy: 0.8494 - val_loss: 2.4708 - val_accuracy: 0.5231\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 4s 469ms/step - loss: 0.4866 - accuracy: 0.8842 - val_loss: 2.7781 - val_accuracy: 0.5077\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 4s 498ms/step - loss: 1.2253 - accuracy: 0.7375 - val_loss: 2.4551 - val_accuracy: 0.5846\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 4s 473ms/step - loss: 0.5380 - accuracy: 0.8494 - val_loss: 2.7171 - val_accuracy: 0.4923\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 4s 470ms/step - loss: 0.4829 - accuracy: 0.8803 - val_loss: 2.4597 - val_accuracy: 0.6308\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 4s 477ms/step - loss: 0.3167 - accuracy: 0.9344 - val_loss: 2.3081 - val_accuracy: 0.5692\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 4s 477ms/step - loss: 0.2430 - accuracy: 0.9537 - val_loss: 2.7317 - val_accuracy: 0.5538\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 4s 479ms/step - loss: 0.1773 - accuracy: 0.9575 - val_loss: 2.6162 - val_accuracy: 0.5538\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 4s 471ms/step - loss: 0.1298 - accuracy: 0.9653 - val_loss: 2.5815 - val_accuracy: 0.6154\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 4s 478ms/step - loss: 0.0751 - accuracy: 0.9846 - val_loss: 2.9261 - val_accuracy: 0.5692\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 4s 467ms/step - loss: 0.1329 - accuracy: 0.9730 - val_loss: 2.5628 - val_accuracy: 0.6000\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 4s 467ms/step - loss: 0.2488 - accuracy: 0.9421 - val_loss: 3.0546 - val_accuracy: 0.5846\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 4s 473ms/step - loss: 0.3751 - accuracy: 0.9112 - val_loss: 2.9792 - val_accuracy: 0.5692\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 4s 492ms/step - loss: 0.2435 - accuracy: 0.9266 - val_loss: 2.4423 - val_accuracy: 0.6462\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 4s 483ms/step - loss: 0.1912 - accuracy: 0.9382 - val_loss: 2.7305 - val_accuracy: 0.6154\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 4s 493ms/step - loss: 0.2426 - accuracy: 0.9305 - val_loss: 3.7150 - val_accuracy: 0.4923\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 4s 485ms/step - loss: 0.9274 - accuracy: 0.8378 - val_loss: 3.2464 - val_accuracy: 0.5231\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 0.5474 - accuracy: 0.8571 - val_loss: 3.7251 - val_accuracy: 0.4308\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 4s 497ms/step - loss: 0.7802 - accuracy: 0.8069 - val_loss: 2.5170 - val_accuracy: 0.6000\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 4s 504ms/step - loss: 0.4838 - accuracy: 0.8417 - val_loss: 2.9274 - val_accuracy: 0.4615\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 0.6438 - accuracy: 0.8147 - val_loss: 2.9389 - val_accuracy: 0.4923\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 4s 481ms/step - loss: 0.6049 - accuracy: 0.8494 - val_loss: 2.6263 - val_accuracy: 0.5231\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 4s 480ms/step - loss: 0.4019 - accuracy: 0.8764 - val_loss: 3.6133 - val_accuracy: 0.5077\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 4s 487ms/step - loss: 0.4619 - accuracy: 0.8842 - val_loss: 2.5696 - val_accuracy: 0.5385\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 0.2307 - accuracy: 0.9266 - val_loss: 2.7053 - val_accuracy: 0.5538\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 4s 506ms/step - loss: 0.2402 - accuracy: 0.9421 - val_loss: 3.3867 - val_accuracy: 0.5077\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 4s 488ms/step - loss: 0.2472 - accuracy: 0.9266 - val_loss: 2.9830 - val_accuracy: 0.5692\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 4s 487ms/step - loss: 0.3107 - accuracy: 0.9421 - val_loss: 3.1910 - val_accuracy: 0.5692\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 4s 485ms/step - loss: 0.1814 - accuracy: 0.9575 - val_loss: 2.9977 - val_accuracy: 0.6000\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 4s 485ms/step - loss: 0.0817 - accuracy: 0.9730 - val_loss: 2.9884 - val_accuracy: 0.5692\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 4s 483ms/step - loss: 0.0605 - accuracy: 0.9807 - val_loss: 3.1512 - val_accuracy: 0.5385\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 4s 500ms/step - loss: 0.0525 - accuracy: 0.9884 - val_loss: 2.8361 - val_accuracy: 0.5692\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 4s 477ms/step - loss: 0.0403 - accuracy: 0.9846 - val_loss: 2.8447 - val_accuracy: 0.5846\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 4s 480ms/step - loss: 0.0180 - accuracy: 0.9961 - val_loss: 2.9824 - val_accuracy: 0.5846\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 4s 478ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 2.9126 - val_accuracy: 0.6000\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 5s 516ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.9592 - val_accuracy: 0.6000\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 4s 499ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 2.9788 - val_accuracy: 0.6000\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 4s 486ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.0119 - val_accuracy: 0.6000\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 4s 500ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.0317 - val_accuracy: 0.6000\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 4s 495ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.0421 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x249176b8ca0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 7s 89ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "           2       1.00      0.50      0.67         2\n",
      "           3       0.50      0.50      0.50         2\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       1.00      0.80      0.89         5\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       1.00      0.67      0.80         3\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       1.00      1.00      1.00         1\n",
      "          11       0.75      0.60      0.67         5\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       1.00      0.33      0.50         3\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.60      1.00      0.75         3\n",
      "          17       0.00      0.00      0.00         3\n",
      "          19       0.00      0.00      0.00         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.67      1.00      0.80         4\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.50      1.00      0.67         4\n",
      "          24       0.75      0.43      0.55         7\n",
      "          25       0.00      0.00      0.00         2\n",
      "          26       1.00      0.50      0.67         2\n",
      "          27       1.00      0.67      0.80         3\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.43      1.00      0.60         3\n",
      "          30       0.75      1.00      0.86         3\n",
      "          31       0.00      0.00      0.00         0\n",
      "          32       1.00      1.00      1.00         1\n",
      "          33       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         1\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.69        81\n",
      "   macro avg       0.62      0.61      0.59        81\n",
      "weighted avg       0.72      0.69      0.68        81\n",
      "\n",
      "Test Loss: 3.0161\n",
      "Test Accuracy: 0.6914\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# hide warning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred_class))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Evaluate Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- Prepare Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23105953c0b643d1abda6029f54d3202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating responses:   0%|          | 0/405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 205ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 231ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "\n",
      "Final Counts:\n",
      "- Questions: 405\n",
      "- Answers: 405\n",
      "- Bot Outputs: 405\n",
      "\n",
      "All questions processed successfully\n"
     ]
    }
   ],
   "source": [
    "# ===== imports ===== \n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# load data\n",
    "with open(\"intents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "for intent in intents_data[\"intents\"]:\n",
    "    for pattern in intent.get(\"patterns\", []):\n",
    "        questions.append(pattern)\n",
    "        answers.append(random.choice(intent[\"responses\"]))\n",
    "\n",
    "# preprocess\n",
    "processed_questions = [preprocess_text_per_word(q) for q in questions]\n",
    "valid_questions = []\n",
    "valid_indices = []\n",
    "for i, q in enumerate(processed_questions):\n",
    "    if q.strip():\n",
    "        valid_questions.append(apply_ner_tags(q))\n",
    "        valid_indices.append(i)\n",
    "valid_answers = [answers[i] for i in valid_indices]\n",
    "\n",
    "# populate bot outputs\n",
    "bot_outputs = []\n",
    "# preprocessed_questions_eval_embed = np.array([get_bert_embedding(sentence, tokenizer, bert_model) for sentence in tqdm(preprocessed_questions_eval)])\n",
    "for i, question in enumerate(tqdm(valid_questions, desc=\"Generating responses\")):\n",
    "    embedded_input = np.expand_dims(get_bert_embedding(question, tokenizer, bert_model), axis=0)\n",
    "    prediction = model.predict(embedded_input)[0]\n",
    "    predicted_class_index = np.argmax(prediction)\n",
    "    tag = label_encoder.inverse_transform([predicted_class_index])[0]\n",
    "\n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            response = random.choice(intent['responses'])\n",
    "            bot_outputs.append(response)\n",
    "            break\n",
    "\n",
    "print(\"\\nFinal Counts:\")\n",
    "print(f\"- Questions: {len(valid_questions)}\")\n",
    "print(f\"- Answers: {len(valid_answers)}\")\n",
    "print(f\"- Bot Outputs: {len(bot_outputs)}\")\n",
    "\n",
    "if len(valid_questions) == len(bot_outputs):\n",
    "    print(\"\\nAll questions processed successfully\")\n",
    "else:\n",
    "    print(\"\\nMismatch in input/output counts!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'Hello!', 'Hello, $_user!', 'Hi there, how can I help?', 'Good to see you again!', 'Talk to you later', 'Hello, $_user!', 'Good to see you again!', 'Hello, $_user!', 'welcome, anything else i can assist you with?', 'Goodbye!', 'Sad to see you go :(', 'Sad to see you go :(', 'Sad to see you go :(', 'Goodbye!', 'You can call me Mind Reader.', 'Come back soon', 'Talk to you later', 'Sad to see you go :(', 'Sad to see you go :(']\n",
      "-------------------------------------------\n",
      "['Hello, $_user!', 'Good to see you again!', 'Hello!', 'Good to see you again!', 'Hi there, how can I help?', 'Hello!', 'Good to see you again!', 'Hello!', 'Good to see you again!', 'Hi there, how can I help?', 'Talk to you later', 'Goodbye!', 'Sad to see you go :(', 'Come back soon', 'Talk to you later', 'Come back soon', 'Come back soon', 'Sad to see you go :(', 'Come back soon', 'Talk to you later']\n"
     ]
    }
   ],
   "source": [
    "print(bot_outputs[:20])\n",
    "print(\"-------------------------------------------\")\n",
    "print(valid_answers[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.8145\n",
      "ROUGE-2: 0.7876\n",
      "ROUGE-L: 0.8117\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Evaluate all\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for ref, hyp in zip(valid_answers, bot_outputs):\n",
    "    scores = scorer.score(ref, hyp)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Average scores\n",
    "avg_r1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_r2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "print(f\"ROUGE-1: {avg_r1:.4f}\")\n",
    "print(f\"ROUGE-2: {avg_r2:.4f}\")\n",
    "print(f\"ROUGE-L: {avg_rL:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Evaluate BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01328b0fdcc145f39abbd42c28b77e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f77d13d8b64916bbfa18695f5dd4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 19.50 seconds, 20.77 sentences/sec\n",
      "\n",
      "BERTScore:\n",
      "Precision: 0.9638\n",
      "Recall:    0.9644\n",
      "F1 Score:  0.9640\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Make sure you have these lists already:\n",
    "# bot_outputs = [your chatbot's answers]\n",
    "# answers = [expected/gold answers]\n",
    "\n",
    "# Run BERTScore\n",
    "P, R, F1 = score(bot_outputs, valid_answers, lang=\"en\", verbose=True)\n",
    "\n",
    "# Average scores\n",
    "avg_precision = P.mean().item()\n",
    "avg_recall = R.mean().item()\n",
    "avg_f1 = F1.mean().item()\n",
    "\n",
    "print(f\"\\nBERTScore:\")\n",
    "print(f\"Precision: {avg_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Save Model and Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "base_name = 'chatbot_campus_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('chatbot_campus_lstm_custom_tokenizer/tokenizer_config.json',\n",
       " 'chatbot_campus_lstm_custom_tokenizer/special_tokens_map.json',\n",
       " 'chatbot_campus_lstm_custom_tokenizer/vocab.txt',\n",
       " 'chatbot_campus_lstm_custom_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Keras model\n",
    "model.save(f\"{base_name}.h5\")\n",
    "\n",
    "# Save label encoder\n",
    "import pickle\n",
    "with open(f\"{base_name}_label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Save tokenizer only if you modified BERT’s tokenizer\n",
    "tokenizer.save_pretrained(f\"{base_name}_custom_tokenizer/\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
