{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Programs\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Imports ------------------- #\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Third-party library imports\n",
    "import jieba\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Input, LSTM, Masking\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Configuration ------------------- #\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_SIZE = 512\n",
    "MAX_LEN = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 406\n",
      "Number of labels: 406\n",
      "Unique labels: {'syllabus', 'greeting', 'floors', 'facilities', 'sports', 'vacation', 'sem', 'uniform', 'hours', 'name', 'hod', 'extchod', 'ithod', 'library', 'location', 'canteen', 'fees', 'scholarship', 'swear', 'course', 'computerhod', 'infrastructure', 'random', 'salutaion', 'hostel', 'principal', 'placement', 'admission', 'event', 'creator', 'document', 'ragging', 'committee', 'menu', 'task', 'number', 'college intake', 'goodbye'}\n",
      "Sample texts: ['Hi', 'Hi', 'How are you?', 'Is anyone there?', 'Hello']\n",
      "Sample labels: ['greeting', 'greeting', 'greeting', 'greeting', 'greeting']\n"
     ]
    }
   ],
   "source": [
    "# Load intents\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare data\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        texts.append(pattern)\n",
    "        labels.append(intent['tag'])\n",
    "        \n",
    "# Summary\n",
    "print(f\"Number of texts: {len(texts)}\")\n",
    "print(f\"Number of labels: {len(labels)}\")\n",
    "print(f\"Unique labels: {set(labels)}\")\n",
    "print(f\"Sample texts: {texts[:5]}\")\n",
    "print(f\"Sample labels: {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Common Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== data summary =====\n",
      "Original number of texts: 406\n",
      "Number of texts after preprocessing: 405\n",
      "Number of skipped texts: 1\n",
      "Sample before preprocessing: ['Hi', 'Hi', 'How are you?', 'Is anyone there?', 'Hello']\n",
      "Sample after preprocessing: ['hi', 'hi', 'how are you?', 'is anyone there?', 'hello']\n",
      "\n",
      "===== label summary =====\n",
      "Original number of labels: 406\n",
      "Number of labels after preprocessing: 405\n",
      "Sample labels before preprocessing: ['greeting', 'greeting', 'greeting', 'greeting', 'greeting']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_text_per_word(text):\n",
    "    words = text.split()\n",
    "    processed_words = [word.lower() for word in words]\n",
    "    processed_words = [word.strip() for word in processed_words]\n",
    "    processed_words = [re.sub('\\s+',' ', word) for word in processed_words]\n",
    "    processed_words = [word for word in processed_words if not all(char in string.punctuation for char in word.replace(' ',''))]\n",
    "    \n",
    "    processed_words = ' '.join(processed_words)\n",
    "    return processed_words\n",
    "\n",
    "# Preprocess texts\n",
    "preprocessed_texts = [preprocess_text_per_word(text) for text in texts]\n",
    "\n",
    "filtered_texts = []\n",
    "skipped_indices = []\n",
    "for i, word in enumerate(preprocessed_texts):\n",
    "    if word.strip() != '':\n",
    "        filtered_texts.append(word)\n",
    "    else:\n",
    "        skipped_indices.append(i)\n",
    "        \n",
    "preprocessed_texts = filtered_texts\n",
    "labels = [label for i, label in enumerate(labels) if i not in skipped_indices]\n",
    "\n",
    "\n",
    "# summary\n",
    "print(\"===== data summary =====\")\n",
    "print(f\"Original number of texts: {len(texts)}\")\n",
    "print(f\"Number of texts after preprocessing: {len(preprocessed_texts)}\")\n",
    "print(f\"Number of skipped texts: {len(skipped_indices)}\")\n",
    "print(\"Sample before preprocessing:\", texts[:5])\n",
    "print(\"Sample after preprocessing:\", preprocessed_texts[:5])\n",
    "print()\n",
    "print(\"===== label summary =====\")\n",
    "print(f\"Original number of labels: {len(labels) + len(skipped_indices)}\")\n",
    "print(f\"Number of labels after preprocessing: {len(labels)}\")\n",
    "print(\"Sample labels before preprocessing:\", labels[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Semantic Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct NER tags: {'ORG', 'TIME', 'DATE', 'ORDINAL', 'PERSON'}\n",
      "Sample tagged texts: ['hi', 'hi', 'how are you ?', 'is anyone there ?', 'hello']\n",
      "Length of tagged texts: 405\n",
      "Length of distinct tags: 5\n"
     ]
    }
   ],
   "source": [
    "distinct_tags = set()\n",
    "\n",
    "def apply_ner_tags(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        distinct_tags.add(ent.label_)\n",
    "        \n",
    "    tagged_tokens = []\n",
    "    for token in doc:\n",
    "        replaced = False\n",
    "        for ent in doc.ents:\n",
    "            if token.text == ent.text:\n",
    "                tagged_tokens.append(f\"<{ent.label_}>\")\n",
    "                replaced = True\n",
    "                break\n",
    "        if not replaced:\n",
    "            tagged_tokens.append(token.text)\n",
    "    return \" \".join(tagged_tokens)\n",
    "\n",
    "tagged_texts = [apply_ner_tags(text) for text in preprocessed_texts]\n",
    "\n",
    "# Summary of distinct tags and tagged texts\n",
    "print(f\"Distinct NER tags: {distinct_tags}\")\n",
    "print(f\"Sample tagged texts: {tagged_texts[:5]}\")\n",
    "print(f\"Length of tagged texts: {len(tagged_texts)}\")\n",
    "print(f\"Length of distinct tags: {len(distinct_tags)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (277, 300)\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "class GloVeEmbeddings:\n",
    "    def __init__(self, embedding_path, tokenizer):\n",
    "        self.word2vec = {}\n",
    "        with open(embedding_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                word = parts[0]\n",
    "                vec = np.array(parts[1:], dtype=np.float32)\n",
    "                self.word2vec[word] = vec\n",
    "\n",
    "        # Create embedding matrix\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "        self.embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            vec = self.word2vec.get(word)\n",
    "            if vec is not None:\n",
    "                self.embedding_matrix[i] = vec\n",
    "\n",
    "    def get_embedding_matrix(self):\n",
    "        return self.embedding_matrix\n",
    "\n",
    "# Example usage\n",
    "glove_path = \"glove.6B.300d.txt\"  # Path to GloVe file\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")  # Assuming tokenizer is already defined\n",
    "tokenizer.fit_on_texts(tagged_texts)  # Fit tokenizer on preprocessed texts\n",
    "\n",
    "glove = GloVeEmbeddings(glove_path, tokenizer)\n",
    "embedding_matrix = glove.get_embedding_matrix()\n",
    "\n",
    "# Summary\n",
    "print(\"tokenization\")\n",
    "print(\"Word Index:\")\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(\"\\nSequences:\")\n",
    "print(sequences)\n",
    "print()\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Feature and Label Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (405, 20)\n",
      "Shape of y: (405, 38)\n",
      "Classes: ['admission' 'canteen' 'college intake' 'committee' 'computerhod' 'course'\n",
      " 'creator' 'document' 'event' 'extchod' 'facilities' 'fees' 'floors'\n",
      " 'goodbye' 'greeting' 'hod' 'hostel' 'hours' 'infrastructure' 'ithod'\n",
      " 'library' 'location' 'menu' 'name' 'number' 'placement' 'principal'\n",
      " 'ragging' 'random' 'salutaion' 'scholarship' 'sem' 'sports' 'swear'\n",
      " 'syllabus' 'task' 'uniform' 'vacation']\n",
      "Shape of X_train: (324, 20)\n",
      "Shape of X_test: (81, 20)\n",
      "Shape of y_train: (324, 38)\n",
      "Shape of y_test: (81, 38)\n"
     ]
    }
   ],
   "source": [
    "# Convert texts to sequences\n",
    "X = tokenizer.texts_to_sequences(tagged_texts)\n",
    "X = pad_sequences(X, maxlen=MAX_LEN)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "y = to_categorical(y, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Summary\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Programs\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Programs\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking (Masking)           (None, 20)                0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 20, 300)           83100     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 20, 512)           1665024   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 20, 256)           787456    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               66048     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 38)                4902      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2967874 (11.32 MB)\n",
      "Trainable params: 2884774 (11.00 MB)\n",
      "Non-trainable params: 83100 (324.61 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# masking layer\n",
    "model.add(Masking(mask_value=0., input_shape=(MAX_LEN,)))\n",
    "\n",
    "# Embedding layer with pre-trained weights\n",
    "model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                    output_dim=embedding_matrix.shape[1],  # EMBEDDING_DIM\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_LEN,\n",
    "                    trainable=False))\n",
    "\n",
    "# LSTM layers\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "\n",
    "# Additional Layers\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From d:\\Programs\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Programs\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "9/9 [==============================] - 34s 822ms/step - loss: 3.5840 - accuracy: 0.0888 - val_loss: 3.5112 - val_accuracy: 0.0769\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 2s 247ms/step - loss: 3.2778 - accuracy: 0.1274 - val_loss: 3.3283 - val_accuracy: 0.0769\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 2s 267ms/step - loss: 3.0075 - accuracy: 0.2008 - val_loss: 3.1578 - val_accuracy: 0.2154\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 3s 391ms/step - loss: 2.7281 - accuracy: 0.2703 - val_loss: 3.0144 - val_accuracy: 0.2462\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 3s 346ms/step - loss: 2.3902 - accuracy: 0.3552 - val_loss: 2.7217 - val_accuracy: 0.3538\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 3s 317ms/step - loss: 1.9948 - accuracy: 0.4556 - val_loss: 2.8096 - val_accuracy: 0.3538\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 3s 343ms/step - loss: 1.7892 - accuracy: 0.4865 - val_loss: 2.8056 - val_accuracy: 0.3077\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 3s 377ms/step - loss: 1.6623 - accuracy: 0.4942 - val_loss: 2.3186 - val_accuracy: 0.4923\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 3s 340ms/step - loss: 1.3738 - accuracy: 0.5560 - val_loss: 2.2239 - val_accuracy: 0.4923\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 3s 349ms/step - loss: 1.0320 - accuracy: 0.6795 - val_loss: 2.0232 - val_accuracy: 0.5385\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 3s 333ms/step - loss: 0.9971 - accuracy: 0.6873 - val_loss: 2.1215 - val_accuracy: 0.5385\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 3s 344ms/step - loss: 0.7868 - accuracy: 0.7413 - val_loss: 2.0032 - val_accuracy: 0.4923\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 3s 386ms/step - loss: 0.6690 - accuracy: 0.7799 - val_loss: 2.2332 - val_accuracy: 0.5692\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 3s 359ms/step - loss: 0.5381 - accuracy: 0.8417 - val_loss: 2.2500 - val_accuracy: 0.5692\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 3s 325ms/step - loss: 0.4395 - accuracy: 0.8417 - val_loss: 2.1253 - val_accuracy: 0.5538\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 5s 535ms/step - loss: 0.3925 - accuracy: 0.8726 - val_loss: 2.3367 - val_accuracy: 0.5846\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 3s 367ms/step - loss: 0.3393 - accuracy: 0.8996 - val_loss: 2.3130 - val_accuracy: 0.5846\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 3s 380ms/step - loss: 0.4201 - accuracy: 0.8880 - val_loss: 2.3865 - val_accuracy: 0.5846\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 2s 264ms/step - loss: 0.2991 - accuracy: 0.9151 - val_loss: 2.4392 - val_accuracy: 0.5692\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 3s 302ms/step - loss: 0.2684 - accuracy: 0.9266 - val_loss: 2.4674 - val_accuracy: 0.5385\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 2s 256ms/step - loss: 0.2634 - accuracy: 0.9189 - val_loss: 2.3151 - val_accuracy: 0.6154\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 3s 337ms/step - loss: 0.2743 - accuracy: 0.9189 - val_loss: 2.2931 - val_accuracy: 0.6615\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 3s 358ms/step - loss: 0.2807 - accuracy: 0.9035 - val_loss: 2.5183 - val_accuracy: 0.6462\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 3s 300ms/step - loss: 0.2638 - accuracy: 0.9151 - val_loss: 2.4124 - val_accuracy: 0.6769\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 2s 256ms/step - loss: 0.1575 - accuracy: 0.9537 - val_loss: 2.5683 - val_accuracy: 0.6308\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 2s 255ms/step - loss: 0.1607 - accuracy: 0.9614 - val_loss: 2.1839 - val_accuracy: 0.6615\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 2s 266ms/step - loss: 0.1141 - accuracy: 0.9575 - val_loss: 2.3683 - val_accuracy: 0.6308\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 3s 302ms/step - loss: 0.1106 - accuracy: 0.9614 - val_loss: 2.3820 - val_accuracy: 0.6615\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 0.0842 - accuracy: 0.9768 - val_loss: 2.5796 - val_accuracy: 0.6615\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 2s 273ms/step - loss: 0.0975 - accuracy: 0.9691 - val_loss: 2.4020 - val_accuracy: 0.6923\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.0785 - accuracy: 0.9768 - val_loss: 2.6401 - val_accuracy: 0.6923\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 0.0635 - accuracy: 0.9884 - val_loss: 2.7670 - val_accuracy: 0.6615\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.1008 - accuracy: 0.9653 - val_loss: 2.6050 - val_accuracy: 0.6615\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 2s 271ms/step - loss: 0.0610 - accuracy: 0.9807 - val_loss: 2.5913 - val_accuracy: 0.6923\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 3s 299ms/step - loss: 0.0482 - accuracy: 0.9923 - val_loss: 2.3779 - val_accuracy: 0.6769\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 3s 282ms/step - loss: 0.0340 - accuracy: 0.9961 - val_loss: 2.4013 - val_accuracy: 0.6923\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 3s 282ms/step - loss: 0.0549 - accuracy: 0.9807 - val_loss: 2.4313 - val_accuracy: 0.6615\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 0.0331 - accuracy: 0.9923 - val_loss: 2.5810 - val_accuracy: 0.6769\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 3s 284ms/step - loss: 0.2801 - accuracy: 0.9614 - val_loss: 2.3141 - val_accuracy: 0.6462\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 2s 276ms/step - loss: 0.0925 - accuracy: 0.9807 - val_loss: 2.2541 - val_accuracy: 0.6154\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 2s 275ms/step - loss: 0.1294 - accuracy: 0.9730 - val_loss: 2.3305 - val_accuracy: 0.7231\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 3s 295ms/step - loss: 0.1246 - accuracy: 0.9691 - val_loss: 2.6631 - val_accuracy: 0.6308\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 2s 275ms/step - loss: 0.1481 - accuracy: 0.9459 - val_loss: 2.7333 - val_accuracy: 0.6769\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 3s 291ms/step - loss: 0.1105 - accuracy: 0.9614 - val_loss: 2.6579 - val_accuracy: 0.6615\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 2s 274ms/step - loss: 0.0616 - accuracy: 0.9768 - val_loss: 2.6390 - val_accuracy: 0.6769\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 2s 264ms/step - loss: 0.0548 - accuracy: 0.9768 - val_loss: 2.7397 - val_accuracy: 0.6615\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 2s 262ms/step - loss: 0.0724 - accuracy: 0.9768 - val_loss: 2.9193 - val_accuracy: 0.6769\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 2s 265ms/step - loss: 0.0710 - accuracy: 0.9807 - val_loss: 2.7915 - val_accuracy: 0.6769\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 3s 290ms/step - loss: 0.0778 - accuracy: 0.9846 - val_loss: 3.0281 - val_accuracy: 0.6462\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 2s 276ms/step - loss: 0.0801 - accuracy: 0.9768 - val_loss: 3.1063 - val_accuracy: 0.6615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23a0a6414b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 4s 102ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       0.67      1.00      0.80         2\n",
      "           3       1.00      0.50      0.67         2\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       0.62      1.00      0.77         5\n",
      "           7       0.67      1.00      0.80         2\n",
      "           8       1.00      1.00      1.00         3\n",
      "          10       1.00      1.00      1.00         1\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       0.33      1.00      0.50         1\n",
      "          14       1.00      0.33      0.50         3\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.75      1.00      0.86         3\n",
      "          17       0.67      0.67      0.67         3\n",
      "          19       0.00      0.00      0.00         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.60      0.75      0.67         4\n",
      "          22       0.50      1.00      0.67         1\n",
      "          23       0.33      0.50      0.40         4\n",
      "          24       1.00      0.43      0.60         7\n",
      "          25       0.50      0.50      0.50         2\n",
      "          26       1.00      0.50      0.67         2\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.67      0.67      0.67         3\n",
      "          30       1.00      1.00      1.00         3\n",
      "          32       1.00      1.00      1.00         1\n",
      "          33       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         1\n",
      "          36       1.00      0.50      0.67         2\n",
      "          37       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.75        81\n",
      "   macro avg       0.74      0.74      0.71        81\n",
      "weighted avg       0.80      0.75      0.74        81\n",
      "\n",
      "Test Loss: 2.1681\n",
      "Test Accuracy: 0.7531\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# hide warning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred_class))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Evaluate Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- Prepare Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30327c2278e44711a99f2ca5b8d6f47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating responses:   0%|          | 0/405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "\n",
      "Final Counts:\n",
      "- Questions: 405\n",
      "- Answers: 405\n",
      "- Bot Outputs: 405\n",
      "\n",
      "All questions processed successfully\n"
     ]
    }
   ],
   "source": [
    "# ===== imports ===== \n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ===== embedding =====\n",
    "def create_padded_seq(sentence, tokenizer, max_len):\n",
    "    \"\"\"Convert text to padded embedding sequence\"\"\"\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    padded_seq = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')[0]\n",
    "    return padded_seq\n",
    "\n",
    "# load data\n",
    "with open(\"intents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "for intent in intents_data[\"intents\"]:\n",
    "    for pattern in intent.get(\"patterns\", []):\n",
    "        questions.append(pattern)\n",
    "        answers.append(random.choice(intent[\"responses\"]))\n",
    "\n",
    "# preprocess\n",
    "processed_questions = [preprocess_text_per_word(q) for q in questions]\n",
    "valid_questions = []\n",
    "valid_indices = []\n",
    "for i, q in enumerate(processed_questions):\n",
    "    if q.strip():\n",
    "        valid_questions.append(apply_ner_tags(q))\n",
    "        valid_indices.append(i)\n",
    "valid_answers = [answers[i] for i in valid_indices]\n",
    "\n",
    "# prepare bot_outputs\n",
    "bot_outputs = []\n",
    "\n",
    "for i, question in enumerate(tqdm(valid_questions, desc=\"Generating responses\")):\n",
    "    # Embed and predict\n",
    "    embedded = create_padded_seq(question, tokenizer, MAX_LEN)\n",
    "    pred = model.predict(np.expand_dims(embedded, axis=0))[0]\n",
    "    tag = label_encoder.inverse_transform([np.argmax(pred)])[0]\n",
    "    \n",
    "    # Get response\n",
    "    for intent in intents_data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            bot_outputs.append(random.choice(intent['responses']))\n",
    "            break\n",
    "        \n",
    "# %% [markdown]\n",
    "# # 5. Results Verification\n",
    "# %%\n",
    "print(\"\\nFinal Counts:\")\n",
    "print(f\"- Questions: {len(valid_questions)}\")\n",
    "print(f\"- Answers: {len(valid_answers)}\")\n",
    "print(f\"- Bot Outputs: {len(bot_outputs)}\")\n",
    "\n",
    "if len(valid_questions) == len(bot_outputs):\n",
    "    print(\"\\nAll questions processed successfully\")\n",
    "else:\n",
    "    print(\"\\nMismatch in input/output counts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good to see you again!', 'Hello!', 'Hi there, how can I help?', 'Our university has canteen with variety of food available', 'Good to see you again!', 'Our university has canteen with variety of food available', 'Hello, $_user!', 'Good to see you again!', 'Good to see you again!', 'Hello!', 'Our university has canteen with variety of food available', 'Our university has canteen with variety of food available', 'Our university has canteen with variety of food available', 'Our university has canteen with variety of food available', 'Hello!', 'Our university has canteen with variety of food available', 'Our university has canteen with variety of food available', 'Our university has canteen with variety of food available', 'Our university has canteen with variety of food available', 'Good to see you again!']\n",
      "-------------------------------------------\n",
      "['Good to see you again!', 'Hello!', 'Hello!', 'Hi there, how can I help?', 'Hi there, how can I help?', 'Hello!', 'Hi there, how can I help?', 'Hello!', 'Hello!', 'Hi there, how can I help?', 'Sad to see you go :(', 'Talk to you later', 'Talk to you later', 'Talk to you later', 'Come back soon', 'Talk to you later', 'Come back soon', 'Talk to you later', 'Talk to you later', 'Talk to you later']\n"
     ]
    }
   ],
   "source": [
    "print(bot_outputs[:20])\n",
    "print(\"-------------------------------------------\")\n",
    "print(valid_answers[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.0608\n",
      "ROUGE-2: 0.0394\n",
      "ROUGE-L: 0.0605\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Evaluate all\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for ref, hyp in zip(valid_answers, bot_outputs):\n",
    "    scores = scorer.score(ref, hyp)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Average scores\n",
    "avg_r1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_r2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "print(f\"ROUGE-1: {avg_r1:.4f}\")\n",
    "print(f\"ROUGE-2: {avg_r2:.4f}\")\n",
    "print(f\"ROUGE-L: {avg_rL:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Evaluate BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7512bbbcc2cc4a7cbcec27daba54bd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295bdd6dddb8423d830eaf5edbc43089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 15.99 seconds, 25.33 sentences/sec\n",
      "\n",
      "BERTScore:\n",
      "Precision: 0.8428\n",
      "Recall:    0.7969\n",
      "F1 Score:  0.8185\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Make sure you have these lists already:\n",
    "# bot_outputs = [your chatbot's answers]\n",
    "# answers = [expected/gold answers]\n",
    "\n",
    "# Run BERTScore\n",
    "P, R, F1 = score(bot_outputs, valid_answers, lang=\"en\", verbose=True)\n",
    "\n",
    "# Average scores\n",
    "avg_precision = P.mean().item()\n",
    "avg_recall = R.mean().item()\n",
    "avg_f1 = F1.mean().item()\n",
    "\n",
    "print(f\"\\nBERTScore:\")\n",
    "print(f\"Precision: {avg_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f}\")\n",
    "print(f\"F1 Score:  {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Save Model and Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "base_name = 'chatbot_campus_lstm_original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save Keras model\n",
    "model.save(f\"{base_name}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save label encoder and onehot encoding\n",
    "mapping = {\n",
    "    \"label_encoder\": label_encoder,\n",
    "    \"classes\": label_encoder.classes_  # ['cat', 'dog', 'bird']\n",
    "}\n",
    "\n",
    "with open(f\"{base_name}_label_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mapping, f)\n",
    "\n",
    "# code to open the mapping\n",
    "# with open(\"label_mapping.pkl\", \"rb\") as f:\n",
    "#     mapping = pickle.load(f)\n",
    "# \n",
    "# new_label = \"dog\"\n",
    "# integer = mapping[\"label_encoder\"].transform([new_label])  # [1]\n",
    "# one_hot = to_categorical(integer, num_classes=len(mapping[\"classes\"]))  # [0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizer\n",
    "with open(f\"{base_name}_glove_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# code to load the tokenizer\n",
    "# with open(\"glove_tokenizer.pkl\", \"rb\") as f:\n",
    "#     tokenizer = pickle.load(f)\n",
    "# \n",
    "# # (2) Convert new text to sequences\n",
    "# new_text = [\"hello stranger\"]\n",
    "# seq = tokenizer.texts_to_sequences(new_text)  # [[1, <OOV>]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
